<?xml version="1.0" encoding="iso-8859-1" standalone="no"?>
<!-- Generated by the JDiff Javadoc doclet -->
<!-- (http://www.jdiff.org) -->
<!-- on Tue Jun 15 06:13:40 GMT 2021 -->

<api
  xmlns:xsi='http://www.w3.org/2001/XMLSchema-instance'
  xsi:noNamespaceSchemaLocation='api.xsd'
  name="Apache Hadoop MapReduce Core 3.3.1"
  jdversion="1.0.9">

<!--  Command line arguments =  -doclet org.apache.hadoop.classification.tools.IncludePublicAnnotationsJDiffDoclet -docletpath /build/source/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/hadoop-annotations.jar:/build/source/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/jdiff.jar -verbose -classpath /build/source/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/classes:/build/source/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/target/hadoop-yarn-client-3.3.1.jar:/maven/log4j/log4j/1.2.17/log4j-1.2.17.jar:/maven/org/eclipse/jetty/websocket/websocket-client/9.4.40.v20210413/websocket-client-9.4.40.v20210413.jar:/maven/org/eclipse/jetty/jetty-client/9.4.40.v20210413/jetty-client-9.4.40.v20210413.jar:/maven/org/eclipse/jetty/jetty-io/9.4.40.v20210413/jetty-io-9.4.40.v20210413.jar:/maven/org/eclipse/jetty/websocket/websocket-common/9.4.40.v20210413/websocket-common-9.4.40.v20210413.jar:/maven/org/eclipse/jetty/websocket/websocket-api/9.4.40.v20210413/websocket-api-9.4.40.v20210413.jar:/build/source/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/target/hadoop-yarn-api-3.3.1.jar:/maven/org/jline/jline/3.9.0/jline-3.9.0.jar:/build/source/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/target/hadoop-yarn-common-3.3.1.jar:/build/source/hadoop-common-project/hadoop-auth/target/hadoop-auth-3.3.1.jar:/maven/com/nimbusds/nimbus-jose-jwt/9.8.1/nimbus-jose-jwt-9.8.1.jar:/maven/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/maven/net/minidev/json-smart/2.4.2/json-smart-2.4.2.jar:/maven/net/minidev/accessors-smart/2.4.2/accessors-smart-2.4.2.jar:/maven/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/maven/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar:/maven/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/maven/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/maven/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/maven/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/maven/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/maven/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/maven/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/maven/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/maven/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/maven/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/maven/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/maven/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar:/maven/org/apache/commons/commons-compress/1.19/commons-compress-1.19.jar:/maven/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/maven/org/eclipse/jetty/jetty-util/9.4.40.v20210413/jetty-util-9.4.40.v20210413.jar:/maven/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/maven/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/maven/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/maven/commons-io/commons-io/2.8.0/commons-io-2.8.0.jar:/maven/com/google/inject/guice/4.0/guice-4.0.jar:/maven/javax/inject/javax.inject/1/javax.inject-1.jar:/maven/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/maven/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/maven/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/maven/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/maven/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/maven/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/maven/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/maven/com/sun/jersey/contribs/jersey-guice/1.19/jersey-guice-1.19.jar:/maven/com/fasterxml/jackson/core/jackson-core/2.10.5/jackson-core-2.10.5.jar:/maven/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.10.5/jackson-module-jaxb-annotations-2.10.5.jar:/maven/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.jar:/maven/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.10.5/jackson-jaxrs-json-provider-2.10.5.jar:/maven/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.10.5/jackson-jaxrs-base-2.10.5.jar:/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/hadoop-hdfs-client-3.3.1.jar:/maven/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/maven/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/maven/com/fasterxml/jackson/core/jackson-annotations/2.10.5/jackson-annotations-2.10.5.jar:/maven/org/eclipse/jetty/jetty-server/9.4.40.v20210413/jetty-server-9.4.40.v20210413.jar:/maven/org/eclipse/jetty/jetty-http/9.4.40.v20210413/jetty-http-9.4.40.v20210413.jar:/maven/org/eclipse/jetty/jetty-util-ajax/9.4.40.v20210413/jetty-util-ajax-9.4.40.v20210413.jar:/maven/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/maven/com/fasterxml/jackson/core/jackson-databind/2.10.5.1/jackson-databind-2.10.5.1.jar:/maven/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/maven/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/maven/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/maven/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/maven/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/maven/org/xerial/snappy/snappy-java/1.1.8.2/snappy-java-1.1.8.2.jar:/build/source/hadoop-common-project/hadoop-common/target/hadoop-common-3.3.1.jar:/maven/org/apache/hadoop/thirdparty/hadoop-shaded-protobuf_3_7/1.1.1/hadoop-shaded-protobuf_3_7-1.1.1.jar:/maven/com/google/guava/guava/27.0-jre/guava-27.0-jre.jar:/maven/com/google/guava/failureaccess/1.0/failureaccess-1.0.jar:/maven/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/maven/org/checkerframework/checker-qual/2.5.2/checker-qual-2.5.2.jar:/maven/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar:/maven/org/codehaus/mojo/animal-sniffer-annotations/1.17/animal-sniffer-annotations-1.17.jar:/maven/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/maven/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/maven/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/maven/commons-net/commons-net/3.6/commons-net-3.6.jar:/maven/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar:/maven/org/eclipse/jetty/jetty-servlet/9.4.40.v20210413/jetty-servlet-9.4.40.v20210413.jar:/maven/org/eclipse/jetty/jetty-security/9.4.40.v20210413/jetty-security-9.4.40.v20210413.jar:/maven/org/eclipse/jetty/jetty-webapp/9.4.40.v20210413/jetty-webapp-9.4.40.v20210413.jar:/maven/org/eclipse/jetty/jetty-xml/9.4.40.v20210413/jetty-xml-9.4.40.v20210413.jar:/maven/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/maven/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/maven/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/maven/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/maven/org/apache/commons/commons-lang3/3.7/commons-lang3-3.7.jar:/maven/org/apache/commons/commons-text/1.4/commons-text-1.4.jar:/maven/com/google/re2j/re2j/1.1/re2j-1.1.jar:/maven/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/maven/com/jcraft/jsch/0.1.55/jsch-0.1.55.jar:/maven/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar:/maven/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar:/maven/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar:/maven/org/apache/zookeeper/zookeeper/3.5.6/zookeeper-3.5.6.jar:/maven/org/apache/zookeeper/zookeeper-jute/3.5.6/zookeeper-jute-3.5.6.jar:/maven/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/maven/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/maven/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/maven/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/maven/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/maven/org/codehaus/woodstox/stax2-api/4.2.1/stax2-api-4.2.1.jar:/maven/com/fasterxml/woodstox/woodstox-core/5.3.0/woodstox-core-5.3.0.jar:/maven/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/maven/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/maven/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar:/build/source/hadoop-common-project/hadoop-annotations/target/hadoop-annotations-3.3.1.jar:/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/maven/com/google/inject/extensions/guice-servlet/4.0/guice-servlet-4.0.jar:/maven/io/netty/netty/3.10.6.Final/netty-3.10.6.Final.jar:/maven/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/maven/org/apache/hadoop/thirdparty/hadoop-shaded-guava/1.1.1/hadoop-shaded-guava-1.1.1.jar:/maven/commons-codec/commons-codec/1.11/commons-codec-1.11.jar:/maven/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/maven/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/maven/xerces/xercesImpl/2.11.0/xercesImpl-2.11.0.jar:/maven/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar -sourcepath /build/source/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java -doclet org.apache.hadoop.classification.tools.IncludePublicAnnotationsJDiffDoclet -docletpath /build/source/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/hadoop-annotations.jar:/build/source/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/jdiff.jar -apidir /build/source/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/site/jdiff/xml -apiname Apache Hadoop MapReduce Core 3.3.1 -->
<package name="org.apache.hadoop.filecache">
  <!-- start class org.apache.hadoop.filecache.DistributedCache -->
  <class name="DistributedCache" extends="org.apache.hadoop.mapreduce.filecache.DistributedCache"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <constructor name="DistributedCache"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="addLocalArchives"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.conf.Configuration"/>
      <param name="str" type="java.lang.String"/>
      <doc>
      <![CDATA[Add a archive that has been localized to the conf.  Used
 by internal DistributedCache code.
 @param conf The conf to modify to contain the localized caches
 @param str a comma separated list of local archives]]>
      </doc>
    </method>
    <method name="addLocalFiles"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.conf.Configuration"/>
      <param name="str" type="java.lang.String"/>
      <doc>
      <![CDATA[Add a file that has been localized to the conf..  Used
 by internal DistributedCache code.
 @param conf The conf to modify to contain the localized caches
 @param str a comma separated list of local files]]>
      </doc>
    </method>
    <method name="createAllSymlink"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="Internal to MapReduce framework.  Use DistributedCacheManager
 instead.">
      <param name="conf" type="org.apache.hadoop.conf.Configuration"/>
      <param name="jobCacheDir" type="java.io.File"/>
      <param name="workDir" type="java.io.File"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[This method create symlinks for all files in a given dir in another
 directory. Currently symlinks cannot be disabled. This is a NO-OP.

 @param conf the configuration
 @param jobCacheDir the target directory for creating symlinks
 @param workDir the directory in which the symlinks are created
 @throws IOException
 @deprecated Internal to MapReduce framework.  Use DistributedCacheManager
 instead.]]>
      </doc>
    </method>
    <method name="getFileStatus" return="org.apache.hadoop.fs.FileStatus"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.conf.Configuration"/>
      <param name="cache" type="java.net.URI"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Returns {@link FileStatus} of a given cache file on hdfs. Internal to
 MapReduce.
 @param conf configuration
 @param cache cache file
 @return <code>FileStatus</code> of a given cache file on hdfs
 @throws IOException]]>
      </doc>
    </method>
    <method name="getTimestamp" return="long"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.conf.Configuration"/>
      <param name="cache" type="java.net.URI"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Returns mtime of a given cache file on hdfs. Internal to MapReduce.
 @param conf configuration
 @param cache cache file
 @return mtime of a given cache file on hdfs
 @throws IOException]]>
      </doc>
    </method>
    <method name="setArchiveTimestamps"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.conf.Configuration"/>
      <param name="timestamps" type="java.lang.String"/>
      <doc>
      <![CDATA[This is to check the timestamp of the archives to be localized.
 Used by internal MapReduce code.
 @param conf Configuration which stores the timestamp's
 @param timestamps comma separated list of timestamps of archives.
 The order should be the same as the order in which the archives are added.]]>
      </doc>
    </method>
    <method name="setFileTimestamps"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.conf.Configuration"/>
      <param name="timestamps" type="java.lang.String"/>
      <doc>
      <![CDATA[This is to check the timestamp of the files to be localized.
 Used by internal MapReduce code.
 @param conf Configuration which stores the timestamp's
 @param timestamps comma separated list of timestamps of files.
 The order should be the same as the order in which the files are added.]]>
      </doc>
    </method>
    <method name="setLocalArchives"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.conf.Configuration"/>
      <param name="str" type="java.lang.String"/>
      <doc>
      <![CDATA[Set the conf to contain the location for localized archives.  Used
 by internal DistributedCache code.
 @param conf The conf to modify to contain the localized caches
 @param str a comma separated list of local archives]]>
      </doc>
    </method>
    <method name="setLocalFiles"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.conf.Configuration"/>
      <param name="str" type="java.lang.String"/>
      <doc>
      <![CDATA[Set the conf to contain the location for localized files.  Used
 by internal DistributedCache code.
 @param conf The conf to modify to contain the localized caches
 @param str a comma separated list of local files]]>
      </doc>
    </method>
    <field name="CACHE_FILES_SIZES" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Warning: {@link #CACHE_FILES_SIZES} is not a *public* constant.
 The variable is kept for M/R 1.x applications, M/R 2.x applications should
 use {@link MRJobConfig#CACHE_FILES_SIZES}]]>
      </doc>
    </field>
    <field name="CACHE_ARCHIVES_SIZES" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Warning: {@link #CACHE_ARCHIVES_SIZES} is not a *public* constant.
 The variable is kept for M/R 1.x applications, M/R 2.x applications should
 use {@link MRJobConfig#CACHE_ARCHIVES_SIZES}]]>
      </doc>
    </field>
    <field name="CACHE_ARCHIVES_TIMESTAMPS" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Warning: {@link #CACHE_ARCHIVES_TIMESTAMPS} is not a *public* constant.
 The variable is kept for M/R 1.x applications, M/R 2.x applications should
 use {@link MRJobConfig#CACHE_ARCHIVES_TIMESTAMPS}]]>
      </doc>
    </field>
    <field name="CACHE_FILES_TIMESTAMPS" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Warning: {@link #CACHE_FILES_TIMESTAMPS} is not a *public* constant.
 The variable is kept for M/R 1.x applications, M/R 2.x applications should
 use {@link MRJobConfig#CACHE_FILE_TIMESTAMPS}]]>
      </doc>
    </field>
    <field name="CACHE_ARCHIVES" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Warning: {@link #CACHE_ARCHIVES} is not a *public* constant.
 The variable is kept for M/R 1.x applications, M/R 2.x applications should
 use {@link MRJobConfig#CACHE_ARCHIVES}]]>
      </doc>
    </field>
    <field name="CACHE_FILES" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Warning: {@link #CACHE_FILES} is not a *public* constant.
 The variable is kept for M/R 1.x applications, M/R 2.x applications should
 use {@link MRJobConfig#CACHE_FILES}]]>
      </doc>
    </field>
    <field name="CACHE_LOCALARCHIVES" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Warning: {@link #CACHE_LOCALARCHIVES} is not a *public* constant.
 The variable is kept for M/R 1.x applications, M/R 2.x applications should
 use {@link MRJobConfig#CACHE_LOCALARCHIVES}]]>
      </doc>
    </field>
    <field name="CACHE_LOCALFILES" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Warning: {@link #CACHE_LOCALFILES} is not a *public* constant.
 The variable is kept for M/R 1.x applications, M/R 2.x applications should
 use {@link MRJobConfig#CACHE_LOCALFILES}]]>
      </doc>
    </field>
    <field name="CACHE_SYMLINK" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Warning: {@link #CACHE_SYMLINK} is not a *public* constant.
 The variable is kept for M/R 1.x applications, M/R 2.x applications should
 use {@link MRJobConfig#CACHE_SYMLINK}]]>
      </doc>
    </field>
    <doc>
    <![CDATA[Distribute application-specific large, read-only files efficiently.

 <p><code>DistributedCache</code> is a facility provided by the Map-Reduce
 framework to cache files (text, archives, jars etc.) needed by applications.
 </p>

 <p>Applications specify the files, via urls (hdfs:// or http://) to be cached
 via the {@link org.apache.hadoop.mapred.JobConf}. The
 <code>DistributedCache</code> assumes that the files specified via urls are
 already present on the {@link FileSystem} at the path specified by the url
 and are accessible by every machine in the cluster.</p>

 <p>The framework will copy the necessary files on to the worker node before
 any tasks for the job are executed on that node. Its efficiency stems from
 the fact that the files are only copied once per job and the ability to
 cache archives which are un-archived on the workers.</p>

 <p><code>DistributedCache</code> can be used to distribute simple, read-only
 data/text files and/or more complex types such as archives, jars etc.
 Archives (zip, tar and tgz/tar.gz files) are un-archived at the worker nodes.
 Jars may be optionally added to the classpath of the tasks, a rudimentary
 software distribution mechanism.  Files have execution permissions.
 In older version of Hadoop Map/Reduce users could optionally ask for symlinks
 to be created in the working directory of the child task.  In the current
 version symlinks are always created.  If the URL does not have a fragment
 the name of the file or directory will be used. If multiple files or
 directories map to the same link name, the last one added, will be used.  All
 others will not even be downloaded.</p>

 <p><code>DistributedCache</code> tracks modification timestamps of the cache
 files. Clearly the cache files should not be modified by the application
 or externally while the job is executing.</p>

 <p>Here is an illustrative example on how to use the
 <code>DistributedCache</code>:</p>
 <p><blockquote><pre>
     // Setting up the cache for the application

     1. Copy the requisite files to the <code>FileSystem</code>:

     $ bin/hadoop fs -copyFromLocal lookup.dat /myapp/lookup.dat
     $ bin/hadoop fs -copyFromLocal map.zip /myapp/map.zip
     $ bin/hadoop fs -copyFromLocal mylib.jar /myapp/mylib.jar
     $ bin/hadoop fs -copyFromLocal mytar.tar /myapp/mytar.tar
     $ bin/hadoop fs -copyFromLocal mytgz.tgz /myapp/mytgz.tgz
     $ bin/hadoop fs -copyFromLocal mytargz.tar.gz /myapp/mytargz.tar.gz

     2. Setup the application's <code>JobConf</code>:

     JobConf job = new JobConf();
     DistributedCache.addCacheFile(new URI("/myapp/lookup.dat#lookup.dat"),
                                   job);
     DistributedCache.addCacheArchive(new URI("/myapp/map.zip"), job);
     DistributedCache.addFileToClassPath(new Path("/myapp/mylib.jar"), job);
     DistributedCache.addCacheArchive(new URI("/myapp/mytar.tar"), job);
     DistributedCache.addCacheArchive(new URI("/myapp/mytgz.tgz"), job);
     DistributedCache.addCacheArchive(new URI("/myapp/mytargz.tar.gz"), job);

     3. Use the cached files in the {@link org.apache.hadoop.mapred.Mapper}
     or {@link org.apache.hadoop.mapred.Reducer}:

     public static class MapClass extends MapReduceBase
     implements Mapper&lt;K, V, K, V&gt; {

       private Path[] localArchives;
       private Path[] localFiles;

       public void configure(JobConf job) {
         // Get the cached archives/files
         File f = new File("./map.zip/some/file/in/zip.txt");
       }

       public void map(K key, V value,
                       OutputCollector&lt;K, V&gt; output, Reporter reporter)
       throws IOException {
         // Use data from the cached archives/files here
         // ...
         // ...
         output.collect(k, v);
       }
     }

 </pre></blockquote>

 It is also very common to use the DistributedCache by using
 {@link org.apache.hadoop.util.GenericOptionsParser}.

 This class includes methods that should be used by users
 (specifically those mentioned in the example above, as well
 as {@link DistributedCache#addArchiveToClassPath(Path, Configuration)}),
 as well as methods intended for use by the MapReduce framework
 (e.g., {@link org.apache.hadoop.mapred.JobClient}).

 @see org.apache.hadoop.mapred.JobConf
 @see org.apache.hadoop.mapred.JobClient
 @see org.apache.hadoop.mapreduce.Job]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.filecache.DistributedCache -->
</package>
<package name="org.apache.hadoop.mapred">
  <!-- start class org.apache.hadoop.mapred.ClusterStatus -->
  <class name="ClusterStatus" extends="java.lang.Object"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.io.Writable"/>
    <method name="getTaskTrackers" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the number of task trackers in the cluster.
 
 @return the number of task trackers in the cluster.]]>
      </doc>
    </method>
    <method name="getActiveTrackerNames" return="java.util.Collection"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the names of task trackers in the cluster.
 
 @return the active task trackers in the cluster.]]>
      </doc>
    </method>
    <method name="getBlacklistedTrackerNames" return="java.util.Collection"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the names of task trackers in the cluster.
 
 @return the blacklisted task trackers in the cluster.]]>
      </doc>
    </method>
    <method name="getGraylistedTrackerNames" return="java.util.Collection"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the names of graylisted task trackers in the cluster.

 The gray list of trackers is no longer available on M/R 2.x. The function
 is kept to be compatible with M/R 1.x applications.

 @return an empty graylisted task trackers in the cluster.]]>
      </doc>
    </method>
    <method name="getGraylistedTrackers" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the number of graylisted task trackers in the cluster.

 The gray list of trackers is no longer available on M/R 2.x. The function
 is kept to be compatible with M/R 1.x applications.

 @return 0 graylisted task trackers in the cluster.]]>
      </doc>
    </method>
    <method name="getBlacklistedTrackers" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the number of blacklisted task trackers in the cluster.
 
 @return the number of blacklisted task trackers in the cluster.]]>
      </doc>
    </method>
    <method name="getNumExcludedNodes" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the number of excluded hosts in the cluster.
 @return the number of excluded hosts in the cluster.]]>
      </doc>
    </method>
    <method name="getTTExpiryInterval" return="long"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the tasktracker expiry interval for the cluster
 @return the expiry interval in msec]]>
      </doc>
    </method>
    <method name="getMapTasks" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the number of currently running map tasks in the cluster.
 
 @return the number of currently running map tasks in the cluster.]]>
      </doc>
    </method>
    <method name="getReduceTasks" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the number of currently running reduce tasks in the cluster.
 
 @return the number of currently running reduce tasks in the cluster.]]>
      </doc>
    </method>
    <method name="getMaxMapTasks" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the maximum capacity for running map tasks in the cluster.
 
 @return the maximum capacity for running map tasks in the cluster.]]>
      </doc>
    </method>
    <method name="getMaxReduceTasks" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the maximum capacity for running reduce tasks in the cluster.
 
 @return the maximum capacity for running reduce tasks in the cluster.]]>
      </doc>
    </method>
    <method name="getJobTrackerStatus" return="org.apache.hadoop.mapreduce.Cluster.JobTrackerStatus"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the JobTracker's status.
 
 @return {@link JobTrackerStatus} of the JobTracker]]>
      </doc>
    </method>
    <method name="getMaxMemory" return="long"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Returns UNINITIALIZED_MEMORY_VALUE (-1)]]>
      </doc>
    </method>
    <method name="getUsedMemory" return="long"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Returns UNINITIALIZED_MEMORY_VALUE (-1)]]>
      </doc>
    </method>
    <method name="getBlackListedTrackersInfo" return="java.util.Collection"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Gets the list of blacklisted trackers along with reasons for blacklisting.
 
 @return the collection of {@link BlackListInfo} objects.]]>
      </doc>
    </method>
    <method name="getJobTrackerState" return="org.apache.hadoop.mapred.JobTracker.State"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the current state of the <code>JobTracker</code>,
 as {@link JobTracker.State}

 {@link JobTracker.State} should no longer be used on M/R 2.x. The function
 is kept to be compatible with M/R 1.x applications.

 @return the invalid state of the <code>JobTracker</code>.]]>
      </doc>
    </method>
    <method name="write"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="out" type="java.io.DataOutput"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="readFields"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="in" type="java.io.DataInput"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <field name="UNINITIALIZED_MEMORY_VALUE" type="long"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <doc>
    <![CDATA[Status information on the current state of the Map-Reduce cluster.
 
 <p><code>ClusterStatus</code> provides clients with information such as:
 <ol>
   <li>
   Size of the cluster. 
   </li>
   <li>
   Name of the trackers. 
   </li>
   <li>
   Task capacity of the cluster. 
   </li>
   <li>
   The number of currently running map and reduce tasks.
   </li>
   <li>
   State of the <code>JobTracker</code>.
   </li>
   <li>
   Details regarding black listed trackers.
   </li>
 </ol>
 
 <p>Clients can query for the latest <code>ClusterStatus</code>, via 
 {@link JobClient#getClusterStatus()}.</p>
 
 @see JobClient]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.ClusterStatus -->
  <!-- start class org.apache.hadoop.mapred.Counters -->
  <class name="Counters" extends="org.apache.hadoop.mapreduce.counters.AbstractCounters"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <constructor name="Counters"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <constructor name="Counters" type="org.apache.hadoop.mapreduce.Counters"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="getGroup" return="org.apache.hadoop.mapred.Counters.Group"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="groupName" type="java.lang.String"/>
    </method>
    <method name="getGroupNames" return="java.util.Collection"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="makeCompactString" return="java.lang.String"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="findCounter" return="org.apache.hadoop.mapred.Counters.Counter"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="group" type="java.lang.String"/>
      <param name="name" type="java.lang.String"/>
    </method>
    <method name="findCounter" return="org.apache.hadoop.mapred.Counters.Counter"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="use {@link #findCounter(String, String)} instead">
      <param name="group" type="java.lang.String"/>
      <param name="id" type="int"/>
      <param name="name" type="java.lang.String"/>
      <doc>
      <![CDATA[Find a counter by using strings
 @param group the name of the group
 @param id the id of the counter within the group (0 to N-1)
 @param name the internal name of the counter
 @return the counter for that name
 @deprecated use {@link #findCounter(String, String)} instead]]>
      </doc>
    </method>
    <method name="incrCounter"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="key" type="java.lang.Enum"/>
      <param name="amount" type="long"/>
      <doc>
      <![CDATA[Increments the specified counter by the specified amount, creating it if
 it didn't already exist.
 @param key identifies a counter
 @param amount amount by which counter is to be incremented]]>
      </doc>
    </method>
    <method name="incrCounter"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="group" type="java.lang.String"/>
      <param name="counter" type="java.lang.String"/>
      <param name="amount" type="long"/>
      <doc>
      <![CDATA[Increments the specified counter by the specified amount, creating it if
 it didn't already exist.
 @param group the name of the group
 @param counter the internal name of the counter
 @param amount amount by which counter is to be incremented]]>
      </doc>
    </method>
    <method name="getCounter" return="long"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="key" type="java.lang.Enum"/>
      <doc>
      <![CDATA[Returns current value of the specified counter, or 0 if the counter
 does not exist.
 @param key the counter enum to lookup
 @return the counter value or 0 if counter not found]]>
      </doc>
    </method>
    <method name="incrAllCounters"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="other" type="org.apache.hadoop.mapred.Counters"/>
      <doc>
      <![CDATA[Increments multiple counters by their amounts in another Counters
 instance.
 @param other the other Counters instance]]>
      </doc>
    </method>
    <method name="size" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="use {@link #countCounters()} instead">
      <doc>
      <![CDATA[@return the total number of counters
 @deprecated use {@link #countCounters()} instead]]>
      </doc>
    </method>
    <method name="sum" return="org.apache.hadoop.mapred.Counters"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="a" type="org.apache.hadoop.mapred.Counters"/>
      <param name="b" type="org.apache.hadoop.mapred.Counters"/>
      <doc>
      <![CDATA[Convenience method for computing the sum of two sets of counters.
 @param a the first counters
 @param b the second counters
 @return a new summed counters object]]>
      </doc>
    </method>
    <method name="log"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="log" type="org.slf4j.Logger"/>
      <doc>
      <![CDATA[Logs the current counter values.
 @param log The log to use.]]>
      </doc>
    </method>
    <method name="makeEscapedCompactString" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Represent the counter in a textual format that can be converted back to
 its object form
 @return the string in the following format
 {(groupName)(group-displayName)[(counterName)(displayName)(value)][]*}*]]>
      </doc>
    </method>
    <method name="fromEscapedCompactString" return="org.apache.hadoop.mapred.Counters"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="compactString" type="java.lang.String"/>
      <exception name="ParseException" type="java.text.ParseException"/>
      <doc>
      <![CDATA[Convert a stringified (by {@link #makeEscapedCompactString()} counter
 representation into a counter object.
 @param compactString to parse
 @return a new counters object
 @throws ParseException]]>
      </doc>
    </method>
    <field name="MAX_COUNTER_LIMIT" type="int"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <field name="MAX_GROUP_LIMIT" type="int"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <doc>
    <![CDATA[A set of named counters.

 <p><code>Counters</code> represent global counters, defined either by the
 Map-Reduce framework or applications. Each <code>Counter</code> can be of
 any {@link Enum} type.</p>

 <p><code>Counters</code> are bunched into {@link Group}s, each comprising of
 counters from a particular <code>Enum</code> class.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.Counters -->
  <!-- start class org.apache.hadoop.mapred.Counters.Counter -->
  <class name="Counters.Counter" extends="java.lang.Object"
    abstract="false"
    static="true" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.mapreduce.Counter"/>
    <constructor name="Counter"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="setDisplayName"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="displayName" type="java.lang.String"/>
    </method>
    <method name="getName" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="getDisplayName" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="getValue" return="long"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="setValue"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="value" type="long"/>
    </method>
    <method name="increment"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="incr" type="long"/>
    </method>
    <method name="write"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="out" type="java.io.DataOutput"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="readFields"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="in" type="java.io.DataInput"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="makeEscapedCompactString" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Returns the compact stringified version of the counter in the format
 [(actual-name)(display-name)(value)]
 @return the stringified result]]>
      </doc>
    </method>
    <method name="contentEquals" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="deprecated, no comment">
      <param name="counter" type="org.apache.hadoop.mapred.Counters.Counter"/>
      <doc>
      <![CDATA[Checks for (content) equality of two (basic) counters
 @param counter to compare
 @return true if content equals
 @deprecated]]>
      </doc>
    </method>
    <method name="getCounter" return="long"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[@return the value of the counter]]>
      </doc>
    </method>
    <method name="getUnderlyingCounter" return="org.apache.hadoop.mapreduce.Counter"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="equals" return="boolean"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="genericRight" type="java.lang.Object"/>
    </method>
    <method name="hashCode" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <doc>
    <![CDATA[A counter record, comprising its name and value.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.Counters.Counter -->
  <!-- start class org.apache.hadoop.mapred.Counters.Group -->
  <class name="Counters.Group" extends="java.lang.Object"
    abstract="false"
    static="true" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.mapreduce.counters.CounterGroupBase"/>
    <constructor name="Group"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
    </constructor>
    <method name="getCounter" return="long"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="counterName" type="java.lang.String"/>
      <doc>
      <![CDATA[@param counterName the name of the counter
 @return the value of the specified counter, or 0 if the counter does
 not exist.]]>
      </doc>
    </method>
    <method name="makeEscapedCompactString" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[@return the compact stringified version of the group in the format
 {(actual-name)(display-name)(value)[][][]} where [] are compact strings
 for the counters within.]]>
      </doc>
    </method>
    <method name="getCounter" return="org.apache.hadoop.mapred.Counters.Counter"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="use {@link #findCounter(String)} instead">
      <param name="id" type="int"/>
      <param name="name" type="java.lang.String"/>
      <doc>
      <![CDATA[Get the counter for the given id and create it if it doesn't exist.
 @param id the numeric id of the counter within the group
 @param name the internal counter name
 @return the counter
 @deprecated use {@link #findCounter(String)} instead]]>
      </doc>
    </method>
    <method name="getCounterForName" return="org.apache.hadoop.mapred.Counters.Counter"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="name" type="java.lang.String"/>
      <doc>
      <![CDATA[Get the counter for the given name and create it if it doesn't exist.
 @param name the internal counter name
 @return the counter]]>
      </doc>
    </method>
    <method name="write"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="out" type="java.io.DataOutput"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="readFields"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="in" type="java.io.DataInput"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="iterator" return="java.util.Iterator"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="getName" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="getDisplayName" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="setDisplayName"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="displayName" type="java.lang.String"/>
    </method>
    <method name="addCounter"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="counter" type="org.apache.hadoop.mapred.Counters.Counter"/>
    </method>
    <method name="addCounter" return="org.apache.hadoop.mapred.Counters.Counter"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="name" type="java.lang.String"/>
      <param name="displayName" type="java.lang.String"/>
      <param name="value" type="long"/>
    </method>
    <method name="findCounter" return="org.apache.hadoop.mapred.Counters.Counter"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="counterName" type="java.lang.String"/>
      <param name="displayName" type="java.lang.String"/>
    </method>
    <method name="findCounter" return="org.apache.hadoop.mapred.Counters.Counter"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="counterName" type="java.lang.String"/>
      <param name="create" type="boolean"/>
    </method>
    <method name="findCounter" return="org.apache.hadoop.mapred.Counters.Counter"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="counterName" type="java.lang.String"/>
    </method>
    <method name="size" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="incrAllCounters"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="rightGroup" type="org.apache.hadoop.mapreduce.counters.CounterGroupBase"/>
    </method>
    <method name="getUnderlyingGroup" return="org.apache.hadoop.mapreduce.counters.CounterGroupBase"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="equals" return="boolean"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="genericRight" type="java.lang.Object"/>
    </method>
    <method name="hashCode" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <doc>
    <![CDATA[<code>Group</code> of counters, comprising of counters from a particular
  counter {@link Enum} class.

  <p><code>Group</code>handles localization of the class name and the
  counter names.</p>]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.Counters.Group -->
  <!-- start class org.apache.hadoop.mapred.FileAlreadyExistsException -->
  <class name="FileAlreadyExistsException" extends="java.io.IOException"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <constructor name="FileAlreadyExistsException"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <constructor name="FileAlreadyExistsException" type="java.lang.String"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <doc>
    <![CDATA[Used when target file already exists for any operation and 
 is not configured to be overwritten.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.FileAlreadyExistsException -->
  <!-- start class org.apache.hadoop.mapred.FileInputFormat -->
  <class name="FileInputFormat" extends="java.lang.Object"
    abstract="true"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.mapred.InputFormat"/>
    <constructor name="FileInputFormat"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="setMinSplitSize"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="minSplitSize" type="long"/>
    </method>
    <method name="isSplitable" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="fs" type="org.apache.hadoop.fs.FileSystem"/>
      <param name="filename" type="org.apache.hadoop.fs.Path"/>
      <doc>
      <![CDATA[Is the given filename splittable? Usually, true, but if the file is
 stream compressed, it will not be.

 The default implementation in <code>FileInputFormat</code> always returns
 true. Implementations that may deal with non-splittable files <i>must</i>
 override this method.

 <code>FileInputFormat</code> implementations can override this and return
 <code>false</code> to ensure that individual input files are never split-up
 so that {@link Mapper}s process entire files.
 
 @param fs the file system that the file is on
 @param filename the file name to check
 @return is this file splitable?]]>
      </doc>
    </method>
    <method name="getRecordReader" return="org.apache.hadoop.mapred.RecordReader"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="split" type="org.apache.hadoop.mapred.InputSplit"/>
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="reporter" type="org.apache.hadoop.mapred.Reporter"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="setInputPathFilter"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="filter" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set a PathFilter to be applied to the input paths for the map-reduce job.

 @param filter the PathFilter class use for filtering the input paths.]]>
      </doc>
    </method>
    <method name="getInputPathFilter" return="org.apache.hadoop.fs.PathFilter"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <doc>
      <![CDATA[Get a PathFilter instance of the filter set for the input paths.

 @return the PathFilter instance set for the job, NULL if none has been set.]]>
      </doc>
    </method>
    <method name="addInputPathRecursively"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="result" type="java.util.List"/>
      <param name="fs" type="org.apache.hadoop.fs.FileSystem"/>
      <param name="path" type="org.apache.hadoop.fs.Path"/>
      <param name="inputFilter" type="org.apache.hadoop.fs.PathFilter"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Add files in the input path recursively into the results.
 @param result
          The List to store all files.
 @param fs
          The FileSystem.
 @param path
          The input path.
 @param inputFilter
          The input filter that can be used to filter files/dirs. 
 @throws IOException]]>
      </doc>
    </method>
    <method name="listStatus" return="org.apache.hadoop.fs.FileStatus[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[List input directories.
 Subclasses may override to, e.g., select only files matching a regular
 expression. 
 
 If security is enabled, this method collects
 delegation tokens from the input paths and adds them to the job's
 credentials.
 @param job the job to list input paths for and attach tokens to.
 @return array of FileStatus objects
 @throws IOException if zero items.]]>
      </doc>
    </method>
    <method name="makeSplit" return="org.apache.hadoop.mapred.FileSplit"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="file" type="org.apache.hadoop.fs.Path"/>
      <param name="start" type="long"/>
      <param name="length" type="long"/>
      <param name="hosts" type="java.lang.String[]"/>
      <doc>
      <![CDATA[A factory that makes the split for this class. It can be overridden
 by sub-classes to make sub-types]]>
      </doc>
    </method>
    <method name="makeSplit" return="org.apache.hadoop.mapred.FileSplit"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="file" type="org.apache.hadoop.fs.Path"/>
      <param name="start" type="long"/>
      <param name="length" type="long"/>
      <param name="hosts" type="java.lang.String[]"/>
      <param name="inMemoryHosts" type="java.lang.String[]"/>
      <doc>
      <![CDATA[A factory that makes the split for this class. It can be overridden
 by sub-classes to make sub-types]]>
      </doc>
    </method>
    <method name="getSplits" return="org.apache.hadoop.mapred.InputSplit[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="numSplits" type="int"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Splits files returned by {@link #listStatus(JobConf)} when
 they're too big.]]>
      </doc>
    </method>
    <method name="computeSplitSize" return="long"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="goalSize" type="long"/>
      <param name="minSize" type="long"/>
      <param name="blockSize" type="long"/>
    </method>
    <method name="getBlockIndex" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="blkLocations" type="org.apache.hadoop.fs.BlockLocation[]"/>
      <param name="offset" type="long"/>
    </method>
    <method name="setInputPaths"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="commaSeparatedPaths" type="java.lang.String"/>
      <doc>
      <![CDATA[Sets the given comma separated paths as the list of inputs 
 for the map-reduce job.
 
 @param conf Configuration of the job
 @param commaSeparatedPaths Comma separated paths to be set as 
        the list of inputs for the map-reduce job.]]>
      </doc>
    </method>
    <method name="addInputPaths"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="commaSeparatedPaths" type="java.lang.String"/>
      <doc>
      <![CDATA[Add the given comma separated paths to the list of inputs for
  the map-reduce job.
 
 @param conf The configuration of the job 
 @param commaSeparatedPaths Comma separated paths to be added to
        the list of inputs for the map-reduce job.]]>
      </doc>
    </method>
    <method name="setInputPaths"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="inputPaths" type="org.apache.hadoop.fs.Path[]"/>
      <doc>
      <![CDATA[Set the array of {@link Path}s as the list of inputs
 for the map-reduce job.
 
 @param conf Configuration of the job. 
 @param inputPaths the {@link Path}s of the input directories/files 
 for the map-reduce job.]]>
      </doc>
    </method>
    <method name="addInputPath"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="path" type="org.apache.hadoop.fs.Path"/>
      <doc>
      <![CDATA[Add a {@link Path} to the list of inputs for the map-reduce job.
 
 @param conf The configuration of the job 
 @param path {@link Path} to be added to the list of inputs for 
            the map-reduce job.]]>
      </doc>
    </method>
    <method name="getInputPaths" return="org.apache.hadoop.fs.Path[]"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <doc>
      <![CDATA[Get the list of input {@link Path}s for the map-reduce job.
 
 @param conf The configuration of the job 
 @return the list of input {@link Path}s for the map-reduce job.]]>
      </doc>
    </method>
    <method name="getSplitHosts" return="java.lang.String[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="blkLocations" type="org.apache.hadoop.fs.BlockLocation[]"/>
      <param name="offset" type="long"/>
      <param name="splitSize" type="long"/>
      <param name="clusterMap" type="org.apache.hadoop.net.NetworkTopology"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[This function identifies and returns the hosts that contribute 
 most for a given split. For calculating the contribution, rack
 locality is treated on par with host locality, so hosts from racks
 that contribute the most are preferred over hosts on racks that 
 contribute less
 @param blkLocations The list of block locations
 @param offset 
 @param splitSize 
 @return an array of hosts that contribute most to this split
 @throws IOException]]>
      </doc>
    </method>
    <field name="LOG" type="org.slf4j.Logger"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <field name="NUM_INPUT_FILES" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <field name="INPUT_DIR_RECURSIVE" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <field name="INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <doc>
    <![CDATA[A base class for file-based {@link InputFormat}.
 
 <p><code>FileInputFormat</code> is the base class for all file-based 
 <code>InputFormat</code>s. This provides a generic implementation of
 {@link #getSplits(JobConf, int)}.

 Implementations of <code>FileInputFormat</code> can also override the
 {@link #isSplitable(FileSystem, Path)} method to prevent input files
 from being split-up in certain situations. Implementations that may
 deal with non-splittable files <i>must</i> override this method, since
 the default implementation assumes splitting is always possible.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.FileInputFormat -->
  <!-- start class org.apache.hadoop.mapred.FileOutputCommitter -->
  <class name="FileOutputCommitter" extends="org.apache.hadoop.mapred.OutputCommitter"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <constructor name="FileOutputCommitter"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="getWorkPath" return="org.apache.hadoop.fs.Path"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="context" type="org.apache.hadoop.mapred.TaskAttemptContext"/>
      <param name="outputPath" type="org.apache.hadoop.fs.Path"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="setupJob"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="context" type="org.apache.hadoop.mapred.JobContext"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="commitJob"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="context" type="org.apache.hadoop.mapred.JobContext"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="cleanupJob"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="context" type="org.apache.hadoop.mapred.JobContext"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="abortJob"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="context" type="org.apache.hadoop.mapred.JobContext"/>
      <param name="runState" type="int"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="setupTask"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="context" type="org.apache.hadoop.mapred.TaskAttemptContext"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="commitTask"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="context" type="org.apache.hadoop.mapred.TaskAttemptContext"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="abortTask"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="context" type="org.apache.hadoop.mapred.TaskAttemptContext"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="needsTaskCommit" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="context" type="org.apache.hadoop.mapred.TaskAttemptContext"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="isRecoverySupported" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="isCommitJobRepeatable" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="context" type="org.apache.hadoop.mapred.JobContext"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="isRecoverySupported" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="context" type="org.apache.hadoop.mapred.JobContext"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="recoverTask"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="context" type="org.apache.hadoop.mapred.TaskAttemptContext"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <field name="LOG" type="org.slf4j.Logger"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <field name="TEMP_DIR_NAME" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Temporary directory name]]>
      </doc>
    </field>
    <field name="SUCCEEDED_FILE_NAME" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <doc>
    <![CDATA[An {@link OutputCommitter} that commits files specified 
 in job output directory i.e. ${mapreduce.output.fileoutputformat.outputdir}.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.FileOutputCommitter -->
  <!-- start class org.apache.hadoop.mapred.FileOutputFormat -->
  <class name="FileOutputFormat" extends="java.lang.Object"
    abstract="true"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.mapred.OutputFormat"/>
    <constructor name="FileOutputFormat"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="setCompressOutput"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="compress" type="boolean"/>
      <doc>
      <![CDATA[Set whether the output of the job is compressed.
 @param conf the {@link JobConf} to modify
 @param compress should the output of the job be compressed?]]>
      </doc>
    </method>
    <method name="getCompressOutput" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <doc>
      <![CDATA[Is the job output compressed?
 @param conf the {@link JobConf} to look in
 @return <code>true</code> if the job output should be compressed,
         <code>false</code> otherwise]]>
      </doc>
    </method>
    <method name="setOutputCompressorClass"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="codecClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the {@link CompressionCodec} to be used to compress job outputs.
 @param conf the {@link JobConf} to modify
 @param codecClass the {@link CompressionCodec} to be used to
                   compress the job outputs]]>
      </doc>
    </method>
    <method name="getOutputCompressorClass" return="java.lang.Class"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="defaultValue" type="java.lang.Class"/>
      <doc>
      <![CDATA[Get the {@link CompressionCodec} for compressing the job outputs.
 @param conf the {@link JobConf} to look in
 @param defaultValue the {@link CompressionCodec} to return if not set
 @return the {@link CompressionCodec} to be used to compress the 
         job outputs
 @throws IllegalArgumentException if the class was specified, but not found]]>
      </doc>
    </method>
    <method name="getRecordWriter" return="org.apache.hadoop.mapred.RecordWriter"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="ignored" type="org.apache.hadoop.fs.FileSystem"/>
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="name" type="java.lang.String"/>
      <param name="progress" type="org.apache.hadoop.util.Progressable"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="checkOutputSpecs"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="ignored" type="org.apache.hadoop.fs.FileSystem"/>
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <exception name="FileAlreadyExistsException" type="org.apache.hadoop.mapred.FileAlreadyExistsException"/>
      <exception name="InvalidJobConfException" type="org.apache.hadoop.mapred.InvalidJobConfException"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="setOutputPath"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="outputDir" type="org.apache.hadoop.fs.Path"/>
      <doc>
      <![CDATA[Set the {@link Path} of the output directory for the map-reduce job.

 @param conf The configuration of the job.
 @param outputDir the {@link Path} of the output directory for 
 the map-reduce job.]]>
      </doc>
    </method>
    <method name="getOutputPath" return="org.apache.hadoop.fs.Path"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <doc>
      <![CDATA[Get the {@link Path} to the output directory for the map-reduce job.
 
 @return the {@link Path} to the output directory for the map-reduce job.
 @see FileOutputFormat#getWorkOutputPath(JobConf)]]>
      </doc>
    </method>
    <method name="getWorkOutputPath" return="org.apache.hadoop.fs.Path"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <doc>
      <![CDATA[Get the {@link Path} to the task's temporary output directory 
  for the map-reduce job
  
 <b id="SideEffectFiles">Tasks' Side-Effect Files</b>
 
 <p><i>Note:</i> The following is valid only if the {@link OutputCommitter}
  is {@link FileOutputCommitter}. If <code>OutputCommitter</code> is not 
  a <code>FileOutputCommitter</code>, the task's temporary output
  directory is same as {@link #getOutputPath(JobConf)} i.e.
  <tt>${mapreduce.output.fileoutputformat.outputdir}$</tt></p>
  
 <p>Some applications need to create/write-to side-files, which differ from
 the actual job-outputs.
 
 <p>In such cases there could be issues with 2 instances of the same TIP 
 (running simultaneously e.g. speculative tasks) trying to open/write-to the
 same file (path) on HDFS. Hence the application-writer will have to pick 
 unique names per task-attempt (e.g. using the attemptid, say 
 <tt>attempt_200709221812_0001_m_000000_0</tt>), not just per TIP.</p> 
 
 <p>To get around this the Map-Reduce framework helps the application-writer 
 out by maintaining a special 
 <tt>${mapreduce.output.fileoutputformat.outputdir}/_temporary/_${taskid}</tt> 
 sub-directory for each task-attempt on HDFS where the output of the 
 task-attempt goes. On successful completion of the task-attempt the files 
 in the <tt>${mapreduce.output.fileoutputformat.outputdir}/_temporary/_${taskid}</tt> (only) 
 are <i>promoted</i> to <tt>${mapreduce.output.fileoutputformat.outputdir}</tt>. Of course, the 
 framework discards the sub-directory of unsuccessful task-attempts. This 
 is completely transparent to the application.</p>
 
 <p>The application-writer can take advantage of this by creating any 
 side-files required in <tt>${mapreduce.task.output.dir}</tt> during execution 
 of his reduce-task i.e. via {@link #getWorkOutputPath(JobConf)}, and the 
 framework will move them out similarly - thus she doesn't have to pick 
 unique paths per task-attempt.</p>
 
 <p><i>Note</i>: the value of <tt>${mapreduce.task.output.dir}</tt> during 
 execution of a particular task-attempt is actually 
 <tt>${mapreduce.output.fileoutputformat.outputdir}/_temporary/_{$taskid}</tt>, and this value is 
 set by the map-reduce framework. So, just create any side-files in the 
 path  returned by {@link #getWorkOutputPath(JobConf)} from map/reduce 
 task to take advantage of this feature.</p>
 
 <p>The entire discussion holds true for maps of jobs with 
 reducer=NONE (i.e. 0 reduces) since output of the map, in that case, 
 goes directly to HDFS.</p> 
 
 @return the {@link Path} to the task's temporary output directory 
 for the map-reduce job.]]>
      </doc>
    </method>
    <method name="getTaskOutputPath" return="org.apache.hadoop.fs.Path"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="name" type="java.lang.String"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Helper function to create the task's temporary output directory and 
 return the path to the task's output file.
 
 @param conf job-configuration
 @param name temporary task-output filename
 @return path to the task's temporary output file
 @throws IOException]]>
      </doc>
    </method>
    <method name="getUniqueName" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="name" type="java.lang.String"/>
      <doc>
      <![CDATA[Helper function to generate a name that is unique for the task.

 <p>The generated name can be used to create custom files from within the
 different tasks for the job, the names for different tasks will not collide
 with each other.</p>

 <p>The given name is postfixed with the task type, 'm' for maps, 'r' for
 reduces and the task partition number. For example, give a name 'test'
 running on the first map o the job the generated name will be
 'test-m-00000'.</p>

 @param conf the configuration for the job.
 @param name the name to make unique.
 @return a unique name accross all tasks of the job.]]>
      </doc>
    </method>
    <method name="getPathForCustomFile" return="org.apache.hadoop.fs.Path"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="name" type="java.lang.String"/>
      <doc>
      <![CDATA[Helper function to generate a {@link Path} for a file that is unique for
 the task within the job output directory.

 <p>The path can be used to create custom files from within the map and
 reduce tasks. The path name will be unique for each task. The path parent
 will be the job output directory.</p>ls

 <p>This method uses the {@link #getUniqueName} method to make the file name
 unique for the task.</p>

 @param conf the configuration for the job.
 @param name the name for the file.
 @return a unique path accross all tasks of the job.]]>
      </doc>
    </method>
    <doc>
    <![CDATA[A base class for {@link OutputFormat}.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.FileOutputFormat -->
  <!-- start class org.apache.hadoop.mapred.FileSplit -->
  <class name="FileSplit" extends="org.apache.hadoop.mapreduce.InputSplit"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.mapred.InputSplitWithLocationInfo"/>
    <constructor name="FileSplit"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
    </constructor>
    <constructor name="FileSplit" type="org.apache.hadoop.fs.Path, long, long, org.apache.hadoop.mapred.JobConf"
      static="false" final="false" visibility="public"
      deprecated="deprecated, no comment">
      <doc>
      <![CDATA[Constructs a split.
 @deprecated
 @param file the file name
 @param start the position of the first byte in the file to process
 @param length the number of bytes in the file to process]]>
      </doc>
    </constructor>
    <constructor name="FileSplit" type="org.apache.hadoop.fs.Path, long, long, java.lang.String[]"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Constructs a split with host information

 @param file the file name
 @param start the position of the first byte in the file to process
 @param length the number of bytes in the file to process
 @param hosts the list of hosts containing the block, possibly null]]>
      </doc>
    </constructor>
    <constructor name="FileSplit" type="org.apache.hadoop.fs.Path, long, long, java.lang.String[], java.lang.String[]"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Constructs a split with host information

 @param file the file name
 @param start the position of the first byte in the file to process
 @param length the number of bytes in the file to process
 @param hosts the list of hosts containing the block, possibly null
 @param inMemoryHosts the list of hosts containing the block in memory]]>
      </doc>
    </constructor>
    <constructor name="FileSplit" type="org.apache.hadoop.mapreduce.lib.input.FileSplit"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="getPath" return="org.apache.hadoop.fs.Path"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[The file containing this split's data.]]>
      </doc>
    </method>
    <method name="getStart" return="long"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[The position of the first byte in the file to process.]]>
      </doc>
    </method>
    <method name="getLength" return="long"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[The number of bytes in the file to process.]]>
      </doc>
    </method>
    <method name="toString" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="write"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="out" type="java.io.DataOutput"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="readFields"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="in" type="java.io.DataInput"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="getLocations" return="java.lang.String[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="getLocationInfo" return="org.apache.hadoop.mapred.SplitLocationInfo[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <doc>
    <![CDATA[A section of an input file.  Returned by {@link
 InputFormat#getSplits(JobConf, int)} and passed to
 {@link InputFormat#getRecordReader(InputSplit,JobConf,Reporter)}.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.FileSplit -->
  <!-- start class org.apache.hadoop.mapred.FixedLengthInputFormat -->
  <class name="FixedLengthInputFormat" extends="org.apache.hadoop.mapred.FileInputFormat"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.mapred.JobConfigurable"/>
    <constructor name="FixedLengthInputFormat"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="setRecordLength"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.conf.Configuration"/>
      <param name="recordLength" type="int"/>
      <doc>
      <![CDATA[Set the length of each record
 @param conf configuration
 @param recordLength the length of a record]]>
      </doc>
    </method>
    <method name="getRecordLength" return="int"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.conf.Configuration"/>
      <doc>
      <![CDATA[Get record length value
 @param conf configuration
 @return the record length, zero means none was set]]>
      </doc>
    </method>
    <method name="configure"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
    </method>
    <method name="getRecordReader" return="org.apache.hadoop.mapred.RecordReader"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="genericSplit" type="org.apache.hadoop.mapred.InputSplit"/>
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="reporter" type="org.apache.hadoop.mapred.Reporter"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="isSplitable" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="fs" type="org.apache.hadoop.fs.FileSystem"/>
      <param name="file" type="org.apache.hadoop.fs.Path"/>
    </method>
    <field name="FIXED_RECORD_LENGTH" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <doc>
    <![CDATA[FixedLengthInputFormat is an input format used to read input files
 which contain fixed length records.  The content of a record need not be
 text.  It can be arbitrary binary data.  Users must configure the record
 length property by calling:
 FixedLengthInputFormat.setRecordLength(conf, recordLength);<br><br> or
 conf.setInt(FixedLengthInputFormat.FIXED_RECORD_LENGTH, recordLength);
 <br><br>
 @see FixedLengthRecordReader]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.FixedLengthInputFormat -->
  <!-- start class org.apache.hadoop.mapred.ID -->
  <class name="ID" extends="org.apache.hadoop.mapreduce.ID"
    abstract="true"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <constructor name="ID" type="int"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[constructs an ID object from the given int]]>
      </doc>
    </constructor>
    <constructor name="ID"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
    </constructor>
    <doc>
    <![CDATA[A general identifier, which internally stores the id
 as an integer. This is the super class of {@link JobID}, 
 {@link TaskID} and {@link TaskAttemptID}.
 
 @see JobID
 @see TaskID
 @see TaskAttemptID]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.ID -->
  <!-- start interface org.apache.hadoop.mapred.InputFormat -->
  <interface name="InputFormat"    abstract="true"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <method name="getSplits" return="org.apache.hadoop.mapred.InputSplit[]"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="numSplits" type="int"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Logically split the set of input files for the job.  
 
 <p>Each {@link InputSplit} is then assigned to an individual {@link Mapper}
 for processing.</p>

 <p><i>Note</i>: The split is a <i>logical</i> split of the inputs and the
 input files are not physically split into chunks. For e.g. a split could
 be <i>&lt;input-file-path, start, offset&gt;</i> tuple.
 
 @param job job configuration.
 @param numSplits the desired number of splits, a hint.
 @return an array of {@link InputSplit}s for the job.]]>
      </doc>
    </method>
    <method name="getRecordReader" return="org.apache.hadoop.mapred.RecordReader"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="split" type="org.apache.hadoop.mapred.InputSplit"/>
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="reporter" type="org.apache.hadoop.mapred.Reporter"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Get the {@link RecordReader} for the given {@link InputSplit}.

 <p>It is the responsibility of the <code>RecordReader</code> to respect
 record boundaries while processing the logical split to present a 
 record-oriented view to the individual task.</p>
 
 @param split the {@link InputSplit}
 @param job the job that this split belongs to
 @return a {@link RecordReader}]]>
      </doc>
    </method>
    <doc>
    <![CDATA[<code>InputFormat</code> describes the input-specification for a 
 Map-Reduce job. 
 
 <p>The Map-Reduce framework relies on the <code>InputFormat</code> of the
 job to:<p>
 <ol>
   <li>
   Validate the input-specification of the job. 
   <li>
   Split-up the input file(s) into logical {@link InputSplit}s, each of 
   which is then assigned to an individual {@link Mapper}.
   </li>
   <li>
   Provide the {@link RecordReader} implementation to be used to glean
   input records from the logical <code>InputSplit</code> for processing by 
   the {@link Mapper}.
   </li>
 </ol>
 
 <p>The default behavior of file-based {@link InputFormat}s, typically 
 sub-classes of {@link FileInputFormat}, is to split the 
 input into <i>logical</i> {@link InputSplit}s based on the total size, in 
 bytes, of the input files. However, the {@link FileSystem} blocksize of  
 the input files is treated as an upper bound for input splits. A lower bound 
 on the split size can be set via 
 <a href="{@docRoot}/../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml#mapreduce.input.fileinputformat.split.minsize">
 mapreduce.input.fileinputformat.split.minsize</a>.</p>
 
 <p>Clearly, logical splits based on input-size is insufficient for many 
 applications since record boundaries are to be respected. In such cases, the
 application has to also implement a {@link RecordReader} on whom lies the
 responsibilty to respect record-boundaries and present a record-oriented
 view of the logical <code>InputSplit</code> to the individual task.

 @see InputSplit
 @see RecordReader
 @see JobClient
 @see FileInputFormat]]>
    </doc>
  </interface>
  <!-- end interface org.apache.hadoop.mapred.InputFormat -->
  <!-- start interface org.apache.hadoop.mapred.InputSplit -->
  <interface name="InputSplit"    abstract="true"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.io.Writable"/>
    <method name="getLength" return="long"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Get the total number of bytes in the data of the <code>InputSplit</code>.
 
 @return the number of bytes in the input split.
 @throws IOException]]>
      </doc>
    </method>
    <method name="getLocations" return="java.lang.String[]"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Get the list of hostnames where the input split is located.
 
 @return list of hostnames where data of the <code>InputSplit</code> is
         located as an array of <code>String</code>s.
 @throws IOException]]>
      </doc>
    </method>
    <doc>
    <![CDATA[<code>InputSplit</code> represents the data to be processed by an 
 individual {@link Mapper}. 

 <p>Typically, it presents a byte-oriented view on the input and is the 
 responsibility of {@link RecordReader} of the job to process this and present
 a record-oriented view.
 
 @see InputFormat
 @see RecordReader]]>
    </doc>
  </interface>
  <!-- end interface org.apache.hadoop.mapred.InputSplit -->
  <!-- start interface org.apache.hadoop.mapred.InputSplitWithLocationInfo -->
  <interface name="InputSplitWithLocationInfo"    abstract="true"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.mapred.InputSplit"/>
    <method name="getLocationInfo" return="org.apache.hadoop.mapred.SplitLocationInfo[]"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Gets info about which nodes the input split is stored on and how it is
 stored at each location.
 
 @return list of <code>SplitLocationInfo</code>s describing how the split
    data is stored at each location. A null value indicates that all the
    locations have the data stored on disk.
 @throws IOException]]>
      </doc>
    </method>
  </interface>
  <!-- end interface org.apache.hadoop.mapred.InputSplitWithLocationInfo -->
  <!-- start class org.apache.hadoop.mapred.InvalidFileTypeException -->
  <class name="InvalidFileTypeException" extends="java.io.IOException"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <constructor name="InvalidFileTypeException"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <constructor name="InvalidFileTypeException" type="java.lang.String"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <doc>
    <![CDATA[Used when file type differs from the desired file type. like 
 getting a file when a directory is expected. Or a wrong file type.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.InvalidFileTypeException -->
  <!-- start class org.apache.hadoop.mapred.InvalidInputException -->
  <class name="InvalidInputException" extends="java.io.IOException"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <constructor name="InvalidInputException" type="java.util.List"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Create the exception with the given list.
 The first element of the list is used as the init cause value.
 @param probs the list of problems to report. this list is not copied.]]>
      </doc>
    </constructor>
    <method name="getProblems" return="java.util.List"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the complete list of the problems reported.
 @return the list of problems, which must not be modified]]>
      </doc>
    </method>
    <method name="getMessage" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get a summary message of the problems found.
 @return the concatenated messages from all of the problems.]]>
      </doc>
    </method>
    <doc>
    <![CDATA[This class wraps a list of problems with the input, so that the user
 can get a list of problems together instead of finding and fixing them one 
 by one.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.InvalidInputException -->
  <!-- start class org.apache.hadoop.mapred.InvalidJobConfException -->
  <class name="InvalidJobConfException" extends="java.io.IOException"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <constructor name="InvalidJobConfException"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <constructor name="InvalidJobConfException" type="java.lang.String"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <constructor name="InvalidJobConfException" type="java.lang.String, java.lang.Throwable"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <constructor name="InvalidJobConfException" type="java.lang.Throwable"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <doc>
    <![CDATA[This exception is thrown when jobconf misses some mendatory attributes
 or value of some attributes is invalid.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.InvalidJobConfException -->
  <!-- start class org.apache.hadoop.mapred.JobClient -->
  <class name="JobClient" extends="org.apache.hadoop.mapreduce.tools.CLI"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="java.lang.AutoCloseable"/>
    <constructor name="JobClient"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Create a job client.]]>
      </doc>
    </constructor>
    <constructor name="JobClient" type="org.apache.hadoop.mapred.JobConf"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Build a job client with the given {@link JobConf}, and connect to the 
 default cluster
 
 @param conf the job configuration.
 @throws IOException]]>
      </doc>
    </constructor>
    <constructor name="JobClient" type="org.apache.hadoop.conf.Configuration"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Build a job client with the given {@link Configuration}, 
 and connect to the default cluster
 
 @param conf the configuration.
 @throws IOException]]>
      </doc>
    </constructor>
    <constructor name="JobClient" type="java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Build a job client, connect to the indicated job tracker.
 
 @param jobTrackAddr the job tracker to connect to.
 @param conf configuration.]]>
      </doc>
    </constructor>
    <method name="init"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Connect to the default cluster
 @param conf the job configuration.
 @throws IOException]]>
      </doc>
    </method>
    <method name="close"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Close the <code>JobClient</code>.]]>
      </doc>
    </method>
    <method name="getFs" return="org.apache.hadoop.fs.FileSystem"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Get a filesystem handle.  We need this to prepare jobs
 for submission to the MapReduce system.
 
 @return the filesystem handle.]]>
      </doc>
    </method>
    <method name="getClusterHandle" return="org.apache.hadoop.mapreduce.Cluster"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get a handle to the Cluster]]>
      </doc>
    </method>
    <method name="submitJob" return="org.apache.hadoop.mapred.RunningJob"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="jobFile" type="java.lang.String"/>
      <exception name="FileNotFoundException" type="java.io.FileNotFoundException"/>
      <exception name="InvalidJobConfException" type="org.apache.hadoop.mapred.InvalidJobConfException"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Submit a job to the MR system.
 
 This returns a handle to the {@link RunningJob} which can be used to track
 the running-job.
 
 @param jobFile the job configuration.
 @return a handle to the {@link RunningJob} which can be used to track the
         running-job.
 @throws FileNotFoundException
 @throws InvalidJobConfException
 @throws IOException]]>
      </doc>
    </method>
    <method name="submitJob" return="org.apache.hadoop.mapred.RunningJob"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <exception name="FileNotFoundException" type="java.io.FileNotFoundException"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Submit a job to the MR system.
 This returns a handle to the {@link RunningJob} which can be used to track
 the running-job.
 
 @param conf the job configuration.
 @return a handle to the {@link RunningJob} which can be used to track the
         running-job.
 @throws FileNotFoundException
 @throws IOException]]>
      </doc>
    </method>
    <method name="getJobInner" return="org.apache.hadoop.mapred.RunningJob"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="jobid" type="org.apache.hadoop.mapred.JobID"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="getJob" return="org.apache.hadoop.mapred.RunningJob"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="jobid" type="org.apache.hadoop.mapred.JobID"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Get an {@link RunningJob} object to track an ongoing job.  Returns
 null if the id does not correspond to any known job.

 @param jobid the jobid of the job.
 @return the {@link RunningJob} handle to track the job, null if the
         <code>jobid</code> doesn't correspond to any known job.
 @throws IOException]]>
      </doc>
    </method>
    <method name="getJob" return="org.apache.hadoop.mapred.RunningJob"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="Applications should rather use {@link #getJob(JobID)}.">
      <param name="jobid" type="java.lang.String"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[@deprecated Applications should rather use {@link #getJob(JobID)}.]]>
      </doc>
    </method>
    <method name="getMapTaskReports" return="org.apache.hadoop.mapred.TaskReport[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="jobId" type="org.apache.hadoop.mapred.JobID"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Get the information of the current state of the map tasks of a job.
 
 @param jobId the job to query.
 @return the list of all of the map tips.
 @throws IOException]]>
      </doc>
    </method>
    <method name="getMapTaskReports" return="org.apache.hadoop.mapred.TaskReport[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="Applications should rather use {@link #getMapTaskReports(JobID)}">
      <param name="jobId" type="java.lang.String"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[@deprecated Applications should rather use {@link #getMapTaskReports(JobID)}]]>
      </doc>
    </method>
    <method name="getReduceTaskReports" return="org.apache.hadoop.mapred.TaskReport[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="jobId" type="org.apache.hadoop.mapred.JobID"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Get the information of the current state of the reduce tasks of a job.
 
 @param jobId the job to query.
 @return the list of all of the reduce tips.
 @throws IOException]]>
      </doc>
    </method>
    <method name="getCleanupTaskReports" return="org.apache.hadoop.mapred.TaskReport[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="jobId" type="org.apache.hadoop.mapred.JobID"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Get the information of the current state of the cleanup tasks of a job.
 
 @param jobId the job to query.
 @return the list of all of the cleanup tips.
 @throws IOException]]>
      </doc>
    </method>
    <method name="getSetupTaskReports" return="org.apache.hadoop.mapred.TaskReport[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="jobId" type="org.apache.hadoop.mapred.JobID"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Get the information of the current state of the setup tasks of a job.
 
 @param jobId the job to query.
 @return the list of all of the setup tips.
 @throws IOException]]>
      </doc>
    </method>
    <method name="getReduceTaskReports" return="org.apache.hadoop.mapred.TaskReport[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="Applications should rather use {@link #getReduceTaskReports(JobID)}">
      <param name="jobId" type="java.lang.String"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[@deprecated Applications should rather use {@link #getReduceTaskReports(JobID)}]]>
      </doc>
    </method>
    <method name="displayTasks"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="jobId" type="org.apache.hadoop.mapred.JobID"/>
      <param name="type" type="java.lang.String"/>
      <param name="state" type="java.lang.String"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Display the information about a job's tasks, of a particular type and
 in a particular state
 
 @param jobId the ID of the job
 @param type the type of the task (map/reduce/setup/cleanup)
 @param state the state of the task 
 (pending/running/completed/failed/killed)
 @throws IOException when there is an error communicating with the master
 @throws IllegalArgumentException if an invalid type/state is passed]]>
      </doc>
    </method>
    <method name="getClusterStatus" return="org.apache.hadoop.mapred.ClusterStatus"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Get status information about the Map-Reduce cluster.
  
 @return the status information about the Map-Reduce cluster as an object
         of {@link ClusterStatus}.
 @throws IOException]]>
      </doc>
    </method>
    <method name="getClusterStatus" return="org.apache.hadoop.mapred.ClusterStatus"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="detailed" type="boolean"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Get status information about the Map-Reduce cluster.
  
 @param  detailed if true then get a detailed status including the
         tracker names
 @return the status information about the Map-Reduce cluster as an object
         of {@link ClusterStatus}.
 @throws IOException]]>
      </doc>
    </method>
    <method name="jobsToComplete" return="org.apache.hadoop.mapred.JobStatus[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Get the jobs that are not completed and not failed.
 
 @return array of {@link JobStatus} for the running/to-be-run jobs.
 @throws IOException]]>
      </doc>
    </method>
    <method name="getAllJobs" return="org.apache.hadoop.mapred.JobStatus[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Get the jobs that are submitted.
 
 @return array of {@link JobStatus} for the submitted jobs.
 @throws IOException]]>
      </doc>
    </method>
    <method name="runJob" return="org.apache.hadoop.mapred.RunningJob"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Utility that submits a job, then polls for progress until the job is
 complete.
 
 @param job the job configuration.
 @throws IOException if the job fails]]>
      </doc>
    </method>
    <method name="monitorAndPrintJob" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="job" type="org.apache.hadoop.mapred.RunningJob"/>
      <exception name="IOException" type="java.io.IOException"/>
      <exception name="InterruptedException" type="java.lang.InterruptedException"/>
      <doc>
      <![CDATA[Monitor a job and print status in real-time as progress is made and tasks 
 fail.
 @param conf the job's configuration
 @param job the job to track
 @return true if the job succeeded
 @throws IOException if communication to the JobTracker fails]]>
      </doc>
    </method>
    <method name="setTaskOutputFilter"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="newValue" type="org.apache.hadoop.mapred.JobClient.TaskStatusFilter"/>
      <doc>
      <![CDATA[Sets the output filter for tasks. only those tasks are printed whose
 output matches the filter. 
 @param newValue task filter.]]>
      </doc>
    </method>
    <method name="getTaskOutputFilter" return="org.apache.hadoop.mapred.JobClient.TaskStatusFilter"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <doc>
      <![CDATA[Get the task output filter out of the JobConf.
 
 @param job the JobConf to examine.
 @return the filter level.]]>
      </doc>
    </method>
    <method name="setTaskOutputFilter"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="newValue" type="org.apache.hadoop.mapred.JobClient.TaskStatusFilter"/>
      <doc>
      <![CDATA[Modify the JobConf to set the task output filter.
 
 @param job the JobConf to modify.
 @param newValue the value to set.]]>
      </doc>
    </method>
    <method name="getTaskOutputFilter" return="org.apache.hadoop.mapred.JobClient.TaskStatusFilter"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Returns task output filter.
 @return task filter.]]>
      </doc>
    </method>
    <method name="getCounter" return="long"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="cntrs" type="org.apache.hadoop.mapreduce.Counters"/>
      <param name="counterGroupName" type="java.lang.String"/>
      <param name="counterName" type="java.lang.String"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="getDefaultMaps" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Get status information about the max available Maps in the cluster.
  
 @return the max available Maps in the cluster
 @throws IOException]]>
      </doc>
    </method>
    <method name="getDefaultReduces" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Get status information about the max available Reduces in the cluster.
  
 @return the max available Reduces in the cluster
 @throws IOException]]>
      </doc>
    </method>
    <method name="getSystemDir" return="org.apache.hadoop.fs.Path"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Grab the jobtracker system directory path where job-specific files are to be placed.
 
 @return the system directory where job-specific files are to be placed.]]>
      </doc>
    </method>
    <method name="isJobDirValid" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="jobDirPath" type="org.apache.hadoop.fs.Path"/>
      <param name="fs" type="org.apache.hadoop.fs.FileSystem"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Checks if the job directory is clean and has all the required components
 for (re) starting the job]]>
      </doc>
    </method>
    <method name="getStagingAreaDir" return="org.apache.hadoop.fs.Path"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Fetch the staging area directory for the application
 
 @return path to staging area directory
 @throws IOException]]>
      </doc>
    </method>
    <method name="getRootQueues" return="org.apache.hadoop.mapred.JobQueueInfo[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Returns an array of queue information objects about root level queues
 configured

 @return the array of root level JobQueueInfo objects
 @throws IOException]]>
      </doc>
    </method>
    <method name="getChildQueues" return="org.apache.hadoop.mapred.JobQueueInfo[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="queueName" type="java.lang.String"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Returns an array of queue information objects about immediate children
 of queue queueName.
 
 @param queueName
 @return the array of immediate children JobQueueInfo objects
 @throws IOException]]>
      </doc>
    </method>
    <method name="getQueues" return="org.apache.hadoop.mapred.JobQueueInfo[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Return an array of queue information objects about all the Job Queues
 configured.
 
 @return Array of JobQueueInfo objects
 @throws IOException]]>
      </doc>
    </method>
    <method name="getJobsFromQueue" return="org.apache.hadoop.mapred.JobStatus[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="queueName" type="java.lang.String"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Gets all the jobs which were added to particular Job Queue
 
 @param queueName name of the Job Queue
 @return Array of jobs present in the job queue
 @throws IOException]]>
      </doc>
    </method>
    <method name="getQueueInfo" return="org.apache.hadoop.mapred.JobQueueInfo"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="queueName" type="java.lang.String"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Gets the queue information associated to a particular Job Queue
 
 @param queueName name of the job queue.
 @return Queue information associated to particular queue.
 @throws IOException]]>
      </doc>
    </method>
    <method name="getQueueAclsForCurrentUser" return="org.apache.hadoop.mapred.QueueAclsInfo[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Gets the Queue ACLs for current user
 @return array of QueueAclsInfo object for current user.
 @throws IOException]]>
      </doc>
    </method>
    <method name="getDelegationToken" return="org.apache.hadoop.security.token.Token"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="renewer" type="org.apache.hadoop.io.Text"/>
      <exception name="IOException" type="java.io.IOException"/>
      <exception name="InterruptedException" type="java.lang.InterruptedException"/>
      <doc>
      <![CDATA[Get a delegation token for the user from the JobTracker.
 @param renewer the user who can renew the token
 @return the new token
 @throws IOException]]>
      </doc>
    </method>
    <method name="renewDelegationToken" return="long"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="Use {@link Token#renew} instead">
      <param name="token" type="org.apache.hadoop.security.token.Token"/>
      <exception name="SecretManager.InvalidToken" type="org.apache.hadoop.security.token.SecretManager.InvalidToken"/>
      <exception name="IOException" type="java.io.IOException"/>
      <exception name="InterruptedException" type="java.lang.InterruptedException"/>
      <doc>
      <![CDATA[Renew a delegation token
 @param token the token to renew
 @return true if the renewal went well
 @throws InvalidToken
 @throws IOException
 @deprecated Use {@link Token#renew} instead]]>
      </doc>
    </method>
    <method name="cancelDelegationToken"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="Use {@link Token#cancel} instead">
      <param name="token" type="org.apache.hadoop.security.token.Token"/>
      <exception name="SecretManager.InvalidToken" type="org.apache.hadoop.security.token.SecretManager.InvalidToken"/>
      <exception name="IOException" type="java.io.IOException"/>
      <exception name="InterruptedException" type="java.lang.InterruptedException"/>
      <doc>
      <![CDATA[Cancel a delegation token from the JobTracker
 @param token the token to cancel
 @throws IOException
 @deprecated Use {@link Token#cancel} instead]]>
      </doc>
    </method>
    <method name="main"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="argv" type="java.lang.String[]"/>
      <exception name="Exception" type="java.lang.Exception"/>
    </method>
    <field name="MAPREDUCE_CLIENT_RETRY_POLICY_ENABLED_KEY" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <field name="MAPREDUCE_CLIENT_RETRY_POLICY_ENABLED_DEFAULT" type="boolean"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <field name="MAPREDUCE_CLIENT_RETRY_POLICY_SPEC_KEY" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <field name="MAPREDUCE_CLIENT_RETRY_POLICY_SPEC_DEFAULT" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <doc>
    <![CDATA[<code>JobClient</code> is the primary interface for the user-job to interact
 with the cluster.
 
 <code>JobClient</code> provides facilities to submit jobs, track their 
 progress, access component-tasks' reports/logs, get the Map-Reduce cluster
 status information etc.
 
 <p>The job submission process involves:
 <ol>
   <li>
   Checking the input and output specifications of the job.
   </li>
   <li>
   Computing the {@link InputSplit}s for the job.
   </li>
   <li>
   Setup the requisite accounting information for the {@link DistributedCache} 
   of the job, if necessary.
   </li>
   <li>
   Copying the job's jar and configuration to the map-reduce system directory 
   on the distributed file-system. 
   </li>
   <li>
   Submitting the job to the cluster and optionally monitoring
   it's status.
   </li>
 </ol>
  
 Normally the user creates the application, describes various facets of the
 job via {@link JobConf} and then uses the <code>JobClient</code> to submit 
 the job and monitor its progress.
 
 <p>Here is an example on how to use <code>JobClient</code>:</p>
 <p><blockquote><pre>
     // Create a new JobConf
     JobConf job = new JobConf(new Configuration(), MyJob.class);
     
     // Specify various job-specific parameters     
     job.setJobName("myjob");
     
     job.setInputPath(new Path("in"));
     job.setOutputPath(new Path("out"));
     
     job.setMapperClass(MyJob.MyMapper.class);
     job.setReducerClass(MyJob.MyReducer.class);

     // Submit the job, then poll for progress until the job is complete
     JobClient.runJob(job);
 </pre></blockquote>
 
 <b id="JobControl">Job Control</b>
 
 <p>At times clients would chain map-reduce jobs to accomplish complex tasks 
 which cannot be done via a single map-reduce job. This is fairly easy since 
 the output of the job, typically, goes to distributed file-system and that 
 can be used as the input for the next job.</p>
 
 <p>However, this also means that the onus on ensuring jobs are complete 
 (success/failure) lies squarely on the clients. In such situations the 
 various job-control options are:
 <ol>
   <li>
   {@link #runJob(JobConf)} : submits the job and returns only after 
   the job has completed.
   </li>
   <li>
   {@link #submitJob(JobConf)} : only submits the job, then poll the 
   returned handle to the {@link RunningJob} to query status and make 
   scheduling decisions.
   </li>
   <li>
   {@link JobConf#setJobEndNotificationURI(String)} : setup a notification
   on job-completion, thus avoiding polling.
   </li>
 </ol>
 
 @see JobConf
 @see ClusterStatus
 @see Tool
 @see DistributedCache]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.JobClient -->
  <!-- start class org.apache.hadoop.mapred.JobConf -->
  <class name="JobConf" extends="org.apache.hadoop.conf.Configuration"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <constructor name="JobConf"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Construct a map/reduce job configuration.]]>
      </doc>
    </constructor>
    <constructor name="JobConf" type="java.lang.Class"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Construct a map/reduce job configuration.
 
 @param exampleClass a class whose containing jar is used as the job's jar.]]>
      </doc>
    </constructor>
    <constructor name="JobConf" type="org.apache.hadoop.conf.Configuration"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Construct a map/reduce job configuration.
 
 @param conf a Configuration whose settings will be inherited.]]>
      </doc>
    </constructor>
    <constructor name="JobConf" type="org.apache.hadoop.conf.Configuration, java.lang.Class"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Construct a map/reduce job configuration.
 
 @param conf a Configuration whose settings will be inherited.
 @param exampleClass a class whose containing jar is used as the job's jar.]]>
      </doc>
    </constructor>
    <constructor name="JobConf" type="java.lang.String"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Construct a map/reduce configuration.

 @param config a Configuration-format XML job description file.]]>
      </doc>
    </constructor>
    <constructor name="JobConf" type="org.apache.hadoop.fs.Path"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Construct a map/reduce configuration.

 @param config a Configuration-format XML job description file.]]>
      </doc>
    </constructor>
    <constructor name="JobConf" type="boolean"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[A new map/reduce configuration where the behavior of reading from the
 default resources can be turned off.
 <p>
 If the parameter {@code loadDefaults} is false, the new instance
 will not load resources from the default files.

 @param loadDefaults specifies whether to load from the default files]]>
      </doc>
    </constructor>
    <method name="getCredentials" return="org.apache.hadoop.security.Credentials"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get credentials for the job.
 @return credentials for the job]]>
      </doc>
    </method>
    <method name="getJar" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the user jar for the map-reduce job.
 
 @return the user jar for the map-reduce job.]]>
      </doc>
    </method>
    <method name="setJar"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="jar" type="java.lang.String"/>
      <doc>
      <![CDATA[Set the user jar for the map-reduce job.
 
 @param jar the user jar for the map-reduce job.]]>
      </doc>
    </method>
    <method name="getJarUnpackPattern" return="java.util.regex.Pattern"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the pattern for jar contents to unpack on the tasktracker]]>
      </doc>
    </method>
    <method name="setJarByClass"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="cls" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the job's jar file by finding an example class location.
 
 @param cls the example class.]]>
      </doc>
    </method>
    <method name="getLocalDirs" return="java.lang.String[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="deleteLocalFiles"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Use MRAsyncDiskService.moveAndDeleteAllVolumes instead.]]>
      </doc>
    </method>
    <method name="deleteLocalFiles"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="subdir" type="java.lang.String"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="getLocalPath" return="org.apache.hadoop.fs.Path"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="pathString" type="java.lang.String"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Constructs a local file name. Files are distributed among configured
 local directories.]]>
      </doc>
    </method>
    <method name="getUser" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the reported username for this job.
 
 @return the username]]>
      </doc>
    </method>
    <method name="setUser"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="user" type="java.lang.String"/>
      <doc>
      <![CDATA[Set the reported username for this job.
 
 @param user the username for this job.]]>
      </doc>
    </method>
    <method name="setKeepFailedTaskFiles"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="keep" type="boolean"/>
      <doc>
      <![CDATA[Set whether the framework should keep the intermediate files for 
 failed tasks.
 
 @param keep <code>true</code> if framework should keep the intermediate files 
             for failed tasks, <code>false</code> otherwise.]]>
      </doc>
    </method>
    <method name="getKeepFailedTaskFiles" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Should the temporary files for failed tasks be kept?
 
 @return should the files be kept?]]>
      </doc>
    </method>
    <method name="setKeepTaskFilesPattern"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="pattern" type="java.lang.String"/>
      <doc>
      <![CDATA[Set a regular expression for task names that should be kept. 
 The regular expression ".*_m_000123_0" would keep the files
 for the first instance of map 123 that ran.
 
 @param pattern the java.util.regex.Pattern to match against the 
        task names.]]>
      </doc>
    </method>
    <method name="getKeepTaskFilesPattern" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the regular expression that is matched against the task names
 to see if we need to keep the files.
 
 @return the pattern as a string, if it was set, othewise null.]]>
      </doc>
    </method>
    <method name="setWorkingDirectory"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="dir" type="org.apache.hadoop.fs.Path"/>
      <doc>
      <![CDATA[Set the current working directory for the default file system.
 
 @param dir the new current working directory.]]>
      </doc>
    </method>
    <method name="getWorkingDirectory" return="org.apache.hadoop.fs.Path"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the current working directory for the default file system.
 
 @return the directory name.]]>
      </doc>
    </method>
    <method name="setNumTasksToExecutePerJvm"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="numTasks" type="int"/>
      <doc>
      <![CDATA[Sets the number of tasks that a spawned task JVM should run
 before it exits
 @param numTasks the number of tasks to execute; defaults to 1;
 -1 signifies no limit]]>
      </doc>
    </method>
    <method name="getNumTasksToExecutePerJvm" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the number of tasks that a spawned JVM should execute]]>
      </doc>
    </method>
    <method name="getInputFormat" return="org.apache.hadoop.mapred.InputFormat"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the {@link InputFormat} implementation for the map-reduce job,
 defaults to {@link TextInputFormat} if not specified explicity.
 
 @return the {@link InputFormat} implementation for the map-reduce job.]]>
      </doc>
    </method>
    <method name="setInputFormat"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="theClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the {@link InputFormat} implementation for the map-reduce job.
 
 @param theClass the {@link InputFormat} implementation for the map-reduce 
                 job.]]>
      </doc>
    </method>
    <method name="getOutputFormat" return="org.apache.hadoop.mapred.OutputFormat"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the {@link OutputFormat} implementation for the map-reduce job,
 defaults to {@link TextOutputFormat} if not specified explicity.
 
 @return the {@link OutputFormat} implementation for the map-reduce job.]]>
      </doc>
    </method>
    <method name="getOutputCommitter" return="org.apache.hadoop.mapred.OutputCommitter"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the {@link OutputCommitter} implementation for the map-reduce job,
 defaults to {@link FileOutputCommitter} if not specified explicitly.
 
 @return the {@link OutputCommitter} implementation for the map-reduce job.]]>
      </doc>
    </method>
    <method name="setOutputCommitter"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="theClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the {@link OutputCommitter} implementation for the map-reduce job.
 
 @param theClass the {@link OutputCommitter} implementation for the map-reduce 
                 job.]]>
      </doc>
    </method>
    <method name="setOutputFormat"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="theClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the {@link OutputFormat} implementation for the map-reduce job.
 
 @param theClass the {@link OutputFormat} implementation for the map-reduce 
                 job.]]>
      </doc>
    </method>
    <method name="setCompressMapOutput"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="compress" type="boolean"/>
      <doc>
      <![CDATA[Should the map outputs be compressed before transfer?
 
 @param compress should the map outputs be compressed?]]>
      </doc>
    </method>
    <method name="getCompressMapOutput" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Are the outputs of the maps be compressed?
 
 @return <code>true</code> if the outputs of the maps are to be compressed,
         <code>false</code> otherwise.]]>
      </doc>
    </method>
    <method name="setMapOutputCompressorClass"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="codecClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the given class as the  {@link CompressionCodec} for the map outputs.
 
 @param codecClass the {@link CompressionCodec} class that will compress  
                   the map outputs.]]>
      </doc>
    </method>
    <method name="getMapOutputCompressorClass" return="java.lang.Class"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="defaultValue" type="java.lang.Class"/>
      <doc>
      <![CDATA[Get the {@link CompressionCodec} for compressing the map outputs.
 
 @param defaultValue the {@link CompressionCodec} to return if not set
 @return the {@link CompressionCodec} class that should be used to compress the 
         map outputs.
 @throws IllegalArgumentException if the class was specified, but not found]]>
      </doc>
    </method>
    <method name="getMapOutputKeyClass" return="java.lang.Class"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the key class for the map output data. If it is not set, use the
 (final) output key class. This allows the map output key class to be
 different than the final output key class.
  
 @return the map output key class.]]>
      </doc>
    </method>
    <method name="setMapOutputKeyClass"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="theClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the key class for the map output data. This allows the user to
 specify the map output key class to be different than the final output
 value class.
 
 @param theClass the map output key class.]]>
      </doc>
    </method>
    <method name="getMapOutputValueClass" return="java.lang.Class"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the value class for the map output data. If it is not set, use the
 (final) output value class This allows the map output value class to be
 different than the final output value class.
  
 @return the map output value class.]]>
      </doc>
    </method>
    <method name="setMapOutputValueClass"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="theClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the value class for the map output data. This allows the user to
 specify the map output value class to be different than the final output
 value class.
 
 @param theClass the map output value class.]]>
      </doc>
    </method>
    <method name="getOutputKeyClass" return="java.lang.Class"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the key class for the job output data.
 
 @return the key class for the job output data.]]>
      </doc>
    </method>
    <method name="setOutputKeyClass"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="theClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the key class for the job output data.
 
 @param theClass the key class for the job output data.]]>
      </doc>
    </method>
    <method name="getOutputKeyComparator" return="org.apache.hadoop.io.RawComparator"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the {@link RawComparator} comparator used to compare keys.
 
 @return the {@link RawComparator} comparator used to compare keys.]]>
      </doc>
    </method>
    <method name="setOutputKeyComparatorClass"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="theClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the {@link RawComparator} comparator used to compare keys.
 
 @param theClass the {@link RawComparator} comparator used to 
                 compare keys.
 @see #setOutputValueGroupingComparator(Class)]]>
      </doc>
    </method>
    <method name="setKeyFieldComparatorOptions"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="keySpec" type="java.lang.String"/>
      <doc>
      <![CDATA[Set the {@link KeyFieldBasedComparator} options used to compare keys.
 
 @param keySpec the key specification of the form -k pos1[,pos2], where,
  pos is of the form f[.c][opts], where f is the number
  of the key field to use, and c is the number of the first character from
  the beginning of the field. Fields and character posns are numbered 
  starting with 1; a character position of zero in pos2 indicates the
  field's last character. If '.c' is omitted from pos1, it defaults to 1
  (the beginning of the field); if omitted from pos2, it defaults to 0 
  (the end of the field). opts are ordering options. The supported options
  are:
    -n, (Sort numerically)
    -r, (Reverse the result of comparison)]]>
      </doc>
    </method>
    <method name="getKeyFieldComparatorOption" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the {@link KeyFieldBasedComparator} options]]>
      </doc>
    </method>
    <method name="setKeyFieldPartitionerOptions"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="keySpec" type="java.lang.String"/>
      <doc>
      <![CDATA[Set the {@link KeyFieldBasedPartitioner} options used for 
 {@link Partitioner}
 
 @param keySpec the key specification of the form -k pos1[,pos2], where,
  pos is of the form f[.c][opts], where f is the number
  of the key field to use, and c is the number of the first character from
  the beginning of the field. Fields and character posns are numbered 
  starting with 1; a character position of zero in pos2 indicates the
  field's last character. If '.c' is omitted from pos1, it defaults to 1
  (the beginning of the field); if omitted from pos2, it defaults to 0 
  (the end of the field).]]>
      </doc>
    </method>
    <method name="getKeyFieldPartitionerOption" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the {@link KeyFieldBasedPartitioner} options]]>
      </doc>
    </method>
    <method name="getCombinerKeyGroupingComparator" return="org.apache.hadoop.io.RawComparator"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the user defined {@link WritableComparable} comparator for
 grouping keys of inputs to the combiner.

 @return comparator set by the user for grouping values.
 @see #setCombinerKeyGroupingComparator(Class) for details.]]>
      </doc>
    </method>
    <method name="getOutputValueGroupingComparator" return="org.apache.hadoop.io.RawComparator"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the user defined {@link WritableComparable} comparator for 
 grouping keys of inputs to the reduce.
 
 @return comparator set by the user for grouping values.
 @see #setOutputValueGroupingComparator(Class) for details.]]>
      </doc>
    </method>
    <method name="setCombinerKeyGroupingComparator"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="theClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the user defined {@link RawComparator} comparator for
 grouping keys in the input to the combiner.

 <p>This comparator should be provided if the equivalence rules for keys
 for sorting the intermediates are different from those for grouping keys
 before each call to
 {@link Reducer#reduce(Object, java.util.Iterator, OutputCollector, Reporter)}.</p>

 <p>For key-value pairs (K1,V1) and (K2,V2), the values (V1, V2) are passed
 in a single call to the reduce function if K1 and K2 compare as equal.</p>

 <p>Since {@link #setOutputKeyComparatorClass(Class)} can be used to control
 how keys are sorted, this can be used in conjunction to simulate
 <i>secondary sort on values</i>.</p>

 <p><i>Note</i>: This is not a guarantee of the combiner sort being
 <i>stable</i> in any sense. (In any case, with the order of available
 map-outputs to the combiner being non-deterministic, it wouldn't make
 that much sense.)</p>

 @param theClass the comparator class to be used for grouping keys for the
 combiner. It should implement <code>RawComparator</code>.
 @see #setOutputKeyComparatorClass(Class)]]>
      </doc>
    </method>
    <method name="setOutputValueGroupingComparator"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="theClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the user defined {@link RawComparator} comparator for 
 grouping keys in the input to the reduce.
 
 <p>This comparator should be provided if the equivalence rules for keys
 for sorting the intermediates are different from those for grouping keys
 before each call to 
 {@link Reducer#reduce(Object, java.util.Iterator, OutputCollector, Reporter)}.</p>
  
 <p>For key-value pairs (K1,V1) and (K2,V2), the values (V1, V2) are passed
 in a single call to the reduce function if K1 and K2 compare as equal.</p>
 
 <p>Since {@link #setOutputKeyComparatorClass(Class)} can be used to control 
 how keys are sorted, this can be used in conjunction to simulate 
 <i>secondary sort on values</i>.</p>
  
 <p><i>Note</i>: This is not a guarantee of the reduce sort being 
 <i>stable</i> in any sense. (In any case, with the order of available 
 map-outputs to the reduce being non-deterministic, it wouldn't make 
 that much sense.)</p>
 
 @param theClass the comparator class to be used for grouping keys. 
                 It should implement <code>RawComparator</code>.
 @see #setOutputKeyComparatorClass(Class)
 @see #setCombinerKeyGroupingComparator(Class)]]>
      </doc>
    </method>
    <method name="getUseNewMapper" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Should the framework use the new context-object code for running
 the mapper?
 @return true, if the new api should be used]]>
      </doc>
    </method>
    <method name="setUseNewMapper"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="flag" type="boolean"/>
      <doc>
      <![CDATA[Set whether the framework should use the new api for the mapper.
 This is the default for jobs submitted with the new Job api.
 @param flag true, if the new api should be used]]>
      </doc>
    </method>
    <method name="getUseNewReducer" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Should the framework use the new context-object code for running
 the reducer?
 @return true, if the new api should be used]]>
      </doc>
    </method>
    <method name="setUseNewReducer"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="flag" type="boolean"/>
      <doc>
      <![CDATA[Set whether the framework should use the new api for the reducer. 
 This is the default for jobs submitted with the new Job api.
 @param flag true, if the new api should be used]]>
      </doc>
    </method>
    <method name="getOutputValueClass" return="java.lang.Class"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the value class for job outputs.
 
 @return the value class for job outputs.]]>
      </doc>
    </method>
    <method name="setOutputValueClass"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="theClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the value class for job outputs.
 
 @param theClass the value class for job outputs.]]>
      </doc>
    </method>
    <method name="getMapperClass" return="java.lang.Class"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the {@link Mapper} class for the job.
 
 @return the {@link Mapper} class for the job.]]>
      </doc>
    </method>
    <method name="setMapperClass"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="theClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the {@link Mapper} class for the job.
 
 @param theClass the {@link Mapper} class for the job.]]>
      </doc>
    </method>
    <method name="getMapRunnerClass" return="java.lang.Class"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the {@link MapRunnable} class for the job.
 
 @return the {@link MapRunnable} class for the job.]]>
      </doc>
    </method>
    <method name="setMapRunnerClass"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="theClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Expert: Set the {@link MapRunnable} class for the job.
 
 Typically used to exert greater control on {@link Mapper}s.
 
 @param theClass the {@link MapRunnable} class for the job.]]>
      </doc>
    </method>
    <method name="getPartitionerClass" return="java.lang.Class"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the {@link Partitioner} used to partition {@link Mapper}-outputs 
 to be sent to the {@link Reducer}s.
 
 @return the {@link Partitioner} used to partition map-outputs.]]>
      </doc>
    </method>
    <method name="setPartitionerClass"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="theClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the {@link Partitioner} class used to partition 
 {@link Mapper}-outputs to be sent to the {@link Reducer}s.
 
 @param theClass the {@link Partitioner} used to partition map-outputs.]]>
      </doc>
    </method>
    <method name="getReducerClass" return="java.lang.Class"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the {@link Reducer} class for the job.
 
 @return the {@link Reducer} class for the job.]]>
      </doc>
    </method>
    <method name="setReducerClass"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="theClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the {@link Reducer} class for the job.
 
 @param theClass the {@link Reducer} class for the job.]]>
      </doc>
    </method>
    <method name="getCombinerClass" return="java.lang.Class"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the user-defined <i>combiner</i> class used to combine map-outputs 
 before being sent to the reducers. Typically the combiner is same as the
 the {@link Reducer} for the job i.e. {@link #getReducerClass()}.
 
 @return the user-defined combiner class used to combine map-outputs.]]>
      </doc>
    </method>
    <method name="setCombinerClass"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="theClass" type="java.lang.Class"/>
      <doc>
      <![CDATA[Set the user-defined <i>combiner</i> class used to combine map-outputs 
 before being sent to the reducers. 
 
 <p>The combiner is an application-specified aggregation operation, which
 can help cut down the amount of data transferred between the 
 {@link Mapper} and the {@link Reducer}, leading to better performance.</p>
 
 <p>The framework may invoke the combiner 0, 1, or multiple times, in both
 the mapper and reducer tasks. In general, the combiner is called as the
 sort/merge result is written to disk. The combiner must:
 <ul>
   <li> be side-effect free</li>
   <li> have the same input and output key types and the same input and 
        output value types</li>
 </ul>
 
 <p>Typically the combiner is same as the <code>Reducer</code> for the  
 job i.e. {@link #setReducerClass(Class)}.</p>
 
 @param theClass the user-defined combiner class used to combine 
                 map-outputs.]]>
      </doc>
    </method>
    <method name="getSpeculativeExecution" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Should speculative execution be used for this job? 
 Defaults to <code>true</code>.
 
 @return <code>true</code> if speculative execution be used for this job,
         <code>false</code> otherwise.]]>
      </doc>
    </method>
    <method name="setSpeculativeExecution"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="speculativeExecution" type="boolean"/>
      <doc>
      <![CDATA[Turn speculative execution on or off for this job. 
 
 @param speculativeExecution <code>true</code> if speculative execution 
                             should be turned on, else <code>false</code>.]]>
      </doc>
    </method>
    <method name="getMapSpeculativeExecution" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Should speculative execution be used for this job for map tasks? 
 Defaults to <code>true</code>.
 
 @return <code>true</code> if speculative execution be 
                           used for this job for map tasks,
         <code>false</code> otherwise.]]>
      </doc>
    </method>
    <method name="setMapSpeculativeExecution"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="speculativeExecution" type="boolean"/>
      <doc>
      <![CDATA[Turn speculative execution on or off for this job for map tasks. 
 
 @param speculativeExecution <code>true</code> if speculative execution 
                             should be turned on for map tasks,
                             else <code>false</code>.]]>
      </doc>
    </method>
    <method name="getReduceSpeculativeExecution" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Should speculative execution be used for this job for reduce tasks? 
 Defaults to <code>true</code>.
 
 @return <code>true</code> if speculative execution be used 
                           for reduce tasks for this job,
         <code>false</code> otherwise.]]>
      </doc>
    </method>
    <method name="setReduceSpeculativeExecution"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="speculativeExecution" type="boolean"/>
      <doc>
      <![CDATA[Turn speculative execution on or off for this job for reduce tasks. 
 
 @param speculativeExecution <code>true</code> if speculative execution 
                             should be turned on for reduce tasks,
                             else <code>false</code>.]]>
      </doc>
    </method>
    <method name="getNumMapTasks" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the configured number of map tasks for this job.
 Defaults to <code>1</code>.
 
 @return the number of map tasks for this job.]]>
      </doc>
    </method>
    <method name="setNumMapTasks"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="n" type="int"/>
      <doc>
      <![CDATA[Set the number of map tasks for this job.
 
 <p><i>Note</i>: This is only a <i>hint</i> to the framework. The actual 
 number of spawned map tasks depends on the number of {@link InputSplit}s 
 generated by the job's {@link InputFormat#getSplits(JobConf, int)}.
  
 A custom {@link InputFormat} is typically used to accurately control 
 the number of map tasks for the job.</p>
 
 <b id="NoOfMaps">How many maps?</b>
 
 <p>The number of maps is usually driven by the total size of the inputs 
 i.e. total number of blocks of the input files.</p>
  
 <p>The right level of parallelism for maps seems to be around 10-100 maps 
 per-node, although it has been set up to 300 or so for very cpu-light map 
 tasks. Task setup takes awhile, so it is best if the maps take at least a 
 minute to execute.</p>
 
 <p>The default behavior of file-based {@link InputFormat}s is to split the 
 input into <i>logical</i> {@link InputSplit}s based on the total size, in 
 bytes, of input files. However, the {@link FileSystem} blocksize of the 
 input files is treated as an upper bound for input splits. A lower bound 
 on the split size can be set via 
 <a href="{@docRoot}/../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml#mapreduce.input.fileinputformat.split.minsize">
 mapreduce.input.fileinputformat.split.minsize</a>.</p>
  
 <p>Thus, if you expect 10TB of input data and have a blocksize of 128MB, 
 you'll end up with 82,000 maps, unless {@link #setNumMapTasks(int)} is 
 used to set it even higher.</p>
 
 @param n the number of map tasks for this job.
 @see InputFormat#getSplits(JobConf, int)
 @see FileInputFormat
 @see FileSystem#getDefaultBlockSize()
 @see FileStatus#getBlockSize()]]>
      </doc>
    </method>
    <method name="getNumReduceTasks" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the configured number of reduce tasks for this job. Defaults to
 <code>1</code>.

 @return the number of reduce tasks for this job.]]>
      </doc>
    </method>
    <method name="setNumReduceTasks"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="n" type="int"/>
      <doc>
      <![CDATA[Set the requisite number of reduce tasks for this job.
 
 <b id="NoOfReduces">How many reduces?</b>
 
 <p>The right number of reduces seems to be <code>0.95</code> or 
 <code>1.75</code> multiplied by (
 <i>available memory for reduce tasks</i>
 (The value of this should be smaller than
 numNodes * yarn.nodemanager.resource.memory-mb
 since the resource of memory is shared by map tasks and other
 applications) /
 <a href="{@docRoot}/../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml#mapreduce.reduce.memory.mb">
 mapreduce.reduce.memory.mb</a>).
 </p>
 
 <p>With <code>0.95</code> all of the reduces can launch immediately and 
 start transfering map outputs as the maps finish. With <code>1.75</code> 
 the faster nodes will finish their first round of reduces and launch a 
 second wave of reduces doing a much better job of load balancing.</p>
 
 <p>Increasing the number of reduces increases the framework overhead, but 
 increases load balancing and lowers the cost of failures.</p>
 
 <p>The scaling factors above are slightly less than whole numbers to 
 reserve a few reduce slots in the framework for speculative-tasks, failures
 etc.</p> 

 <b id="ReducerNone">Reducer NONE</b>
 
 <p>It is legal to set the number of reduce-tasks to <code>zero</code>.</p>
 
 <p>In this case the output of the map-tasks directly go to distributed 
 file-system, to the path set by 
 {@link FileOutputFormat#setOutputPath(JobConf, Path)}. Also, the 
 framework doesn't sort the map-outputs before writing it out to HDFS.</p>
 
 @param n the number of reduce tasks for this job.]]>
      </doc>
    </method>
    <method name="getMaxMapAttempts" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the configured number of maximum attempts that will be made to run a
 map task, as specified by the <code>mapreduce.map.maxattempts</code>
 property. If this property is not already set, the default is 4 attempts.
  
 @return the max number of attempts per map task.]]>
      </doc>
    </method>
    <method name="setMaxMapAttempts"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="n" type="int"/>
      <doc>
      <![CDATA[Expert: Set the number of maximum attempts that will be made to run a
 map task.
 
 @param n the number of attempts per map task.]]>
      </doc>
    </method>
    <method name="getMaxReduceAttempts" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the configured number of maximum attempts  that will be made to run a
 reduce task, as specified by the <code>mapreduce.reduce.maxattempts</code>
 property. If this property is not already set, the default is 4 attempts.
 
 @return the max number of attempts per reduce task.]]>
      </doc>
    </method>
    <method name="setMaxReduceAttempts"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="n" type="int"/>
      <doc>
      <![CDATA[Expert: Set the number of maximum attempts that will be made to run a
 reduce task.
 
 @param n the number of attempts per reduce task.]]>
      </doc>
    </method>
    <method name="getJobName" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the user-specified job name. This is only used to identify the 
 job to the user.
 
 @return the job's name, defaulting to "".]]>
      </doc>
    </method>
    <method name="setJobName"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="name" type="java.lang.String"/>
      <doc>
      <![CDATA[Set the user-specified job name.
 
 @param name the job's new name.]]>
      </doc>
    </method>
    <method name="getSessionId" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the user-specified session identifier. The default is the empty string.

 The session identifier is used to tag metric data that is reported to some
 performance metrics system via the org.apache.hadoop.metrics API.  The 
 session identifier is intended, in particular, for use by Hadoop-On-Demand 
 (HOD) which allocates a virtual Hadoop cluster dynamically and transiently. 
 HOD will set the session identifier by modifying the mapred-site.xml file 
 before starting the cluster.

 When not running under HOD, this identifer is expected to remain set to 
 the empty string.

 @return the session identifier, defaulting to "".]]>
      </doc>
    </method>
    <method name="setSessionId"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="sessionId" type="java.lang.String"/>
      <doc>
      <![CDATA[Set the user-specified session identifier.  

 @param sessionId the new session id.]]>
      </doc>
    </method>
    <method name="setMaxTaskFailuresPerTracker"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="noFailures" type="int"/>
      <doc>
      <![CDATA[Set the maximum no. of failures of a given job per tasktracker.
 If the no. of task failures exceeds <code>noFailures</code>, the 
 tasktracker is <i>blacklisted</i> for this job. 
 
 @param noFailures maximum no. of failures of a given job per tasktracker.]]>
      </doc>
    </method>
    <method name="getMaxTaskFailuresPerTracker" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Expert: Get the maximum no. of failures of a given job per tasktracker.
 If the no. of task failures exceeds this, the tasktracker is
 <i>blacklisted</i> for this job. 
 
 @return the maximum no. of failures of a given job per tasktracker.]]>
      </doc>
    </method>
    <method name="getMaxMapTaskFailuresPercent" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the maximum percentage of map tasks that can fail without 
 the job being aborted. 
 
 Each map task is executed a minimum of {@link #getMaxMapAttempts()} 
 attempts before being declared as <i>failed</i>.
  
 Defaults to <code>zero</code>, i.e. <i>any</i> failed map-task results in
 the job being declared as {@link JobStatus#FAILED}.
 
 @return the maximum percentage of map tasks that can fail without
         the job being aborted.]]>
      </doc>
    </method>
    <method name="setMaxMapTaskFailuresPercent"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="percent" type="int"/>
      <doc>
      <![CDATA[Expert: Set the maximum percentage of map tasks that can fail without the
 job being aborted. 
 
 Each map task is executed a minimum of {@link #getMaxMapAttempts} attempts 
 before being declared as <i>failed</i>.
 
 @param percent the maximum percentage of map tasks that can fail without 
                the job being aborted.]]>
      </doc>
    </method>
    <method name="getMaxReduceTaskFailuresPercent" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the maximum percentage of reduce tasks that can fail without 
 the job being aborted. 
 
 Each reduce task is executed a minimum of {@link #getMaxReduceAttempts()} 
 attempts before being declared as <i>failed</i>.
 
 Defaults to <code>zero</code>, i.e. <i>any</i> failed reduce-task results 
 in the job being declared as {@link JobStatus#FAILED}.
 
 @return the maximum percentage of reduce tasks that can fail without
         the job being aborted.]]>
      </doc>
    </method>
    <method name="setMaxReduceTaskFailuresPercent"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="percent" type="int"/>
      <doc>
      <![CDATA[Set the maximum percentage of reduce tasks that can fail without the job
 being aborted.
 
 Each reduce task is executed a minimum of {@link #getMaxReduceAttempts()} 
 attempts before being declared as <i>failed</i>.
 
 @param percent the maximum percentage of reduce tasks that can fail without 
                the job being aborted.]]>
      </doc>
    </method>
    <method name="setJobPriority"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="prio" type="org.apache.hadoop.mapred.JobPriority"/>
      <doc>
      <![CDATA[Set {@link JobPriority} for this job.

 @param prio the {@link JobPriority} for this job.]]>
      </doc>
    </method>
    <method name="setJobPriorityAsInteger"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="prio" type="int"/>
      <doc>
      <![CDATA[Set {@link JobPriority} for this job.

 @param prio the {@link JobPriority} for this job.]]>
      </doc>
    </method>
    <method name="getJobPriority" return="org.apache.hadoop.mapred.JobPriority"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the {@link JobPriority} for this job.

 @return the {@link JobPriority} for this job.]]>
      </doc>
    </method>
    <method name="getJobPriorityAsInteger" return="int"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the priority for this job.

 @return the priority for this job.]]>
      </doc>
    </method>
    <method name="getProfileEnabled" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get whether the task profiling is enabled.
 @return true if some tasks will be profiled]]>
      </doc>
    </method>
    <method name="setProfileEnabled"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="newValue" type="boolean"/>
      <doc>
      <![CDATA[Set whether the system should collect profiler information for some of 
 the tasks in this job? The information is stored in the user log 
 directory.
 @param newValue true means it should be gathered]]>
      </doc>
    </method>
    <method name="getProfileParams" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the profiler configuration arguments.

 The default value for this property is
 "-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s"
 
 @return the parameters to pass to the task child to configure profiling]]>
      </doc>
    </method>
    <method name="setProfileParams"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="value" type="java.lang.String"/>
      <doc>
      <![CDATA[Set the profiler configuration arguments. If the string contains a '%s' it
 will be replaced with the name of the profiling output file when the task
 runs.

 This value is passed to the task child JVM on the command line.

 @param value the configuration string]]>
      </doc>
    </method>
    <method name="getProfileTaskRange" return="org.apache.hadoop.conf.Configuration.IntegerRanges"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="isMap" type="boolean"/>
      <doc>
      <![CDATA[Get the range of maps or reduces to profile.
 @param isMap is the task a map?
 @return the task ranges]]>
      </doc>
    </method>
    <method name="setProfileTaskRange"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="isMap" type="boolean"/>
      <param name="newValue" type="java.lang.String"/>
      <doc>
      <![CDATA[Set the ranges of maps or reduces to profile. setProfileEnabled(true) 
 must also be called.
 @param newValue a set of integer ranges of the map ids]]>
      </doc>
    </method>
    <method name="setMapDebugScript"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="mDbgScript" type="java.lang.String"/>
      <doc>
      <![CDATA[Set the debug script to run when the map tasks fail.
 
 <p>The debug script can aid debugging of failed map tasks. The script is 
 given task's stdout, stderr, syslog, jobconf files as arguments.</p>
 
 <p>The debug command, run on the node where the map failed, is:</p>
 <p><blockquote><pre>
 $script $stdout $stderr $syslog $jobconf.
 </pre></blockquote>
 
 <p> The script file is distributed through {@link DistributedCache} 
 APIs. The script needs to be symlinked. </p>
 
 <p>Here is an example on how to submit a script 
 <p><blockquote><pre>
 job.setMapDebugScript("./myscript");
 DistributedCache.createSymlink(job);
 DistributedCache.addCacheFile("/debug/scripts/myscript#myscript");
 </pre></blockquote>
 
 @param mDbgScript the script name]]>
      </doc>
    </method>
    <method name="getMapDebugScript" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the map task's debug script.
 
 @return the debug Script for the mapred job for failed map tasks.
 @see #setMapDebugScript(String)]]>
      </doc>
    </method>
    <method name="setReduceDebugScript"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="rDbgScript" type="java.lang.String"/>
      <doc>
      <![CDATA[Set the debug script to run when the reduce tasks fail.
 
 <p>The debug script can aid debugging of failed reduce tasks. The script
 is given task's stdout, stderr, syslog, jobconf files as arguments.</p>
 
 <p>The debug command, run on the node where the map failed, is:</p>
 <p><blockquote><pre>
 $script $stdout $stderr $syslog $jobconf.
 </pre></blockquote>
 
 <p> The script file is distributed through {@link DistributedCache} 
 APIs. The script file needs to be symlinked </p>
 
 <p>Here is an example on how to submit a script 
 <p><blockquote><pre>
 job.setReduceDebugScript("./myscript");
 DistributedCache.createSymlink(job);
 DistributedCache.addCacheFile("/debug/scripts/myscript#myscript");
 </pre></blockquote>
 
 @param rDbgScript the script name]]>
      </doc>
    </method>
    <method name="getReduceDebugScript" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the reduce task's debug Script
 
 @return the debug script for the mapred job for failed reduce tasks.
 @see #setReduceDebugScript(String)]]>
      </doc>
    </method>
    <method name="getJobEndNotificationURI" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the uri to be invoked in-order to send a notification after the job 
 has completed (success/failure). 
 
 @return the job end notification uri, <code>null</code> if it hasn't
         been set.
 @see #setJobEndNotificationURI(String)]]>
      </doc>
    </method>
    <method name="setJobEndNotificationURI"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="uri" type="java.lang.String"/>
      <doc>
      <![CDATA[Set the uri to be invoked in-order to send a notification after the job
 has completed (success/failure).
 
 <p>The uri can contain 2 special parameters: <tt>$jobId</tt> and 
 <tt>$jobStatus</tt>. Those, if present, are replaced by the job's 
 identifier and completion-status respectively.</p>
 
 <p>This is typically used by application-writers to implement chaining of 
 Map-Reduce jobs in an <i>asynchronous manner</i>.</p>
 
 @param uri the job end notification uri
 @see JobStatus]]>
      </doc>
    </method>
    <method name="getJobEndNotificationCustomNotifierClass" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Returns the class to be invoked in order to send a notification
 after the job has completed (success/failure).

 @return the fully-qualified name of the class which implements
 {@link org.apache.hadoop.mapreduce.CustomJobEndNotifier} set through the
 {@link org.apache.hadoop.mapreduce.MRJobConfig#MR_JOB_END_NOTIFICATION_CUSTOM_NOTIFIER_CLASS}
 property

 @see JobConf#setJobEndNotificationCustomNotifierClass(java.lang.String)
 @see org.apache.hadoop.mapreduce.MRJobConfig#MR_JOB_END_NOTIFICATION_CUSTOM_NOTIFIER_CLASS]]>
      </doc>
    </method>
    <method name="setJobEndNotificationCustomNotifierClass"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="customNotifierClassName" type="java.lang.String"/>
      <doc>
      <![CDATA[Sets the class to be invoked in order to send a notification after the job
 has completed (success/failure).

 A notification url still has to be set which will be passed to
 {@link org.apache.hadoop.mapreduce.CustomJobEndNotifier#notifyOnce(
 java.net.URL, org.apache.hadoop.conf.Configuration)}
 along with the Job's conf.

 If this is set instead of using a simple HttpURLConnection
 we'll create a new instance of this class
 which should be an implementation of
 {@link org.apache.hadoop.mapreduce.CustomJobEndNotifier},
 and we'll invoke that.

 @param customNotifierClassName the fully-qualified name of the class
     which implements
     {@link org.apache.hadoop.mapreduce.CustomJobEndNotifier}

 @see JobConf#setJobEndNotificationURI(java.lang.String)
 @see
 org.apache.hadoop.mapreduce.MRJobConfig#MR_JOB_END_NOTIFICATION_CUSTOM_NOTIFIER_CLASS]]>
      </doc>
    </method>
    <method name="getJobLocalDir" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get job-specific shared directory for use as scratch space
 
 <p>
 When a job starts, a shared directory is created at location
 <code>
 ${mapreduce.cluster.local.dir}/taskTracker/$user/jobcache/$jobid/work/ </code>.
 This directory is exposed to the users through 
 <code>mapreduce.job.local.dir </code>.
 So, the tasks can use this space 
 as scratch space and share files among them. </p>
 This value is available as System property also.
 
 @return The localized job specific shared directory]]>
      </doc>
    </method>
    <method name="getMemoryForMapTask" return="long"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get memory required to run a map task of the job, in MB.
 
 If a value is specified in the configuration, it is returned.
 Else, it returns {@link JobContext#DEFAULT_MAP_MEMORY_MB}.
 <p>
 For backward compatibility, if the job configuration sets the
 key {@link #MAPRED_TASK_MAXVMEM_PROPERTY} to a value different
 from {@link #DISABLED_MEMORY_LIMIT}, that value will be used
 after converting it from bytes to MB.
 @return memory required to run a map task of the job, in MB,]]>
      </doc>
    </method>
    <method name="setMemoryForMapTask"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="mem" type="long"/>
    </method>
    <method name="getMemoryForReduceTask" return="long"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get memory required to run a reduce task of the job, in MB.
 
 If a value is specified in the configuration, it is returned.
 Else, it returns {@link JobContext#DEFAULT_REDUCE_MEMORY_MB}.
 <p>
 For backward compatibility, if the job configuration sets the
 key {@link #MAPRED_TASK_MAXVMEM_PROPERTY} to a value different
 from {@link #DISABLED_MEMORY_LIMIT}, that value will be used
 after converting it from bytes to MB.
 @return memory required to run a reduce task of the job, in MB.]]>
      </doc>
    </method>
    <method name="setMemoryForReduceTask"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="mem" type="long"/>
    </method>
    <method name="getQueueName" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Return the name of the queue to which this job is submitted.
 Defaults to 'default'.
 
 @return name of the queue]]>
      </doc>
    </method>
    <method name="setQueueName"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="queueName" type="java.lang.String"/>
      <doc>
      <![CDATA[Set the name of the queue to which this job should be submitted.
 
 @param queueName Name of the queue]]>
      </doc>
    </method>
    <method name="normalizeMemoryConfigValue" return="long"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="val" type="long"/>
      <doc>
      <![CDATA[Normalize the negative values in configuration
 
 @param val
 @return normalized value]]>
      </doc>
    </method>
    <method name="findContainingJar" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="my_class" type="java.lang.Class"/>
      <doc>
      <![CDATA[Find a jar that contains a class of the same name, if any.
 It will return a jar file, even if that is not the first thing
 on the class path that has a class with the same name.
 
 @param my_class the class to find.
 @return a jar file that contains the class, or null.]]>
      </doc>
    </method>
    <method name="getMaxVirtualMemoryForTask" return="long"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="Use {@link #getMemoryForMapTask()} and
             {@link #getMemoryForReduceTask()}">
      <doc>
      <![CDATA[Get the memory required to run a task of this job, in bytes. See
 {@link #MAPRED_TASK_MAXVMEM_PROPERTY}
 <p>
 This method is deprecated. Now, different memory limits can be
 set for map and reduce tasks of a job, in MB. 
 <p>
 For backward compatibility, if the job configuration sets the
 key {@link #MAPRED_TASK_MAXVMEM_PROPERTY}, that value is returned. 
 Otherwise, this method will return the larger of the values returned by 
 {@link #getMemoryForMapTask()} and {@link #getMemoryForReduceTask()}
 after converting them into bytes.

 @return Memory required to run a task of this job, in bytes.
 @see #setMaxVirtualMemoryForTask(long)
 @deprecated Use {@link #getMemoryForMapTask()} and
             {@link #getMemoryForReduceTask()}]]>
      </doc>
    </method>
    <method name="setMaxVirtualMemoryForTask"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="Use {@link #setMemoryForMapTask(long mem)}  and
  Use {@link #setMemoryForReduceTask(long mem)}">
      <param name="vmem" type="long"/>
      <doc>
      <![CDATA[Set the maximum amount of memory any task of this job can use. See
 {@link #MAPRED_TASK_MAXVMEM_PROPERTY}
 <p>
 mapred.task.maxvmem is split into
 mapreduce.map.memory.mb
 and mapreduce.map.memory.mb,mapred
 each of the new key are set
 as mapred.task.maxvmem / 1024
 as new values are in MB

 @param vmem Maximum amount of virtual memory in bytes any task of this job
             can use.
 @see #getMaxVirtualMemoryForTask()
 @deprecated
  Use {@link #setMemoryForMapTask(long mem)}  and
  Use {@link #setMemoryForReduceTask(long mem)}]]>
      </doc>
    </method>
    <method name="getMaxPhysicalMemoryForTask" return="long"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="this variable is deprecated and nolonger in use.">
      <doc>
      <![CDATA[@deprecated this variable is deprecated and nolonger in use.]]>
      </doc>
    </method>
    <method name="setMaxPhysicalMemoryForTask"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="mem" type="long"/>
    </method>
    <method name="main"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="args" type="java.lang.String[]"/>
      <exception name="Exception" type="java.lang.Exception"/>
    </method>
    <field name="MAPRED_TASK_MAXVMEM_PROPERTY" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="Use {@link #MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY} and
 {@link #MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY}">
      <doc>
      <![CDATA[@deprecated Use {@link #MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY} and
 {@link #MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY}]]>
      </doc>
    </field>
    <field name="UPPER_LIMIT_ON_TASK_VMEM_PROPERTY" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="deprecated, no comment">
      <doc>
      <![CDATA[@deprecated]]>
      </doc>
    </field>
    <field name="MAPRED_TASK_DEFAULT_MAXVMEM_PROPERTY" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="deprecated, no comment">
      <doc>
      <![CDATA[@deprecated]]>
      </doc>
    </field>
    <field name="MAPRED_TASK_MAXPMEM_PROPERTY" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="deprecated, no comment">
      <doc>
      <![CDATA[@deprecated]]>
      </doc>
    </field>
    <field name="DISABLED_MEMORY_LIMIT" type="long"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[A value which if set for memory related configuration options,
 indicates that the options are turned off.
 Deprecated because it makes no sense in the context of MR2.]]>
      </doc>
    </field>
    <field name="MAPRED_LOCAL_DIR_PROPERTY" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Property name for the configuration property mapreduce.cluster.local.dir]]>
      </doc>
    </field>
    <field name="DEFAULT_QUEUE_NAME" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Name of the queue to which jobs will be submitted, if no queue
 name is mentioned.]]>
      </doc>
    </field>
    <field name="MAPRED_JOB_MAP_MEMORY_MB_PROPERTY" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[The variable is kept for M/R 1.x applications, while M/R 2.x applications
 should use {@link #MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY}]]>
      </doc>
    </field>
    <field name="MAPRED_JOB_REDUCE_MEMORY_MB_PROPERTY" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[The variable is kept for M/R 1.x applications, while M/R 2.x applications
 should use {@link #MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY}]]>
      </doc>
    </field>
    <field name="UNPACK_JAR_PATTERN_DEFAULT" type="java.util.regex.Pattern"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Pattern for the default unpacking behavior for job jars]]>
      </doc>
    </field>
    <field name="MAPRED_TASK_JAVA_OPTS" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="Use {@link #MAPRED_MAP_TASK_JAVA_OPTS} or 
                 {@link #MAPRED_REDUCE_TASK_JAVA_OPTS}">
      <doc>
      <![CDATA[Configuration key to set the java command line options for the child
 map and reduce tasks.
 
 Java opts for the task tracker child processes.
 The following symbol, if present, will be interpolated: @taskid@. 
 It is replaced by current TaskID. Any other occurrences of '@' will go 
 unchanged.
 For example, to enable verbose gc logging to a file named for the taskid in
 /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of:
          -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc
 
 The configuration variable {@link #MAPRED_TASK_ENV} can be used to pass 
 other environment variables to the child processes.
 
 @deprecated Use {@link #MAPRED_MAP_TASK_JAVA_OPTS} or 
                 {@link #MAPRED_REDUCE_TASK_JAVA_OPTS}]]>
      </doc>
    </field>
    <field name="MAPRED_MAP_TASK_JAVA_OPTS" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Configuration key to set the java command line options for the map tasks.
 
 Java opts for the task tracker child map processes.
 The following symbol, if present, will be interpolated: @taskid@. 
 It is replaced by current TaskID. Any other occurrences of '@' will go 
 unchanged.
 For example, to enable verbose gc logging to a file named for the taskid in
 /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of:
          -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc
 
 The configuration variable {@link #MAPRED_MAP_TASK_ENV} can be used to pass 
 other environment variables to the map processes.]]>
      </doc>
    </field>
    <field name="MAPRED_REDUCE_TASK_JAVA_OPTS" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Configuration key to set the java command line options for the reduce tasks.
 
 Java opts for the task tracker child reduce processes.
 The following symbol, if present, will be interpolated: @taskid@. 
 It is replaced by current TaskID. Any other occurrences of '@' will go 
 unchanged.
 For example, to enable verbose gc logging to a file named for the taskid in
 /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of:
          -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc
 
 The configuration variable {@link #MAPRED_REDUCE_TASK_ENV} can be used to 
 pass process environment variables to the reduce processes.]]>
      </doc>
    </field>
    <field name="DEFAULT_MAPRED_TASK_JAVA_OPTS" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <field name="MAPRED_TASK_ULIMIT" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="Configuration key to set the maximum virtual memory available to the child
 map and reduce tasks (in kilo-bytes). This has been deprecated and will no
 longer have any effect.">
      <doc>
      <![CDATA[@deprecated
 Configuration key to set the maximum virtual memory available to the child
 map and reduce tasks (in kilo-bytes). This has been deprecated and will no
 longer have any effect.]]>
      </doc>
    </field>
    <field name="MAPRED_MAP_TASK_ULIMIT" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="Configuration key to set the maximum virtual memory available to the
 map tasks (in kilo-bytes). This has been deprecated and will no
 longer have any effect.">
      <doc>
      <![CDATA[@deprecated
 Configuration key to set the maximum virtual memory available to the
 map tasks (in kilo-bytes). This has been deprecated and will no
 longer have any effect.]]>
      </doc>
    </field>
    <field name="MAPRED_REDUCE_TASK_ULIMIT" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="Configuration key to set the maximum virtual memory available to the
 reduce tasks (in kilo-bytes). This has been deprecated and will no
 longer have any effect.">
      <doc>
      <![CDATA[@deprecated
 Configuration key to set the maximum virtual memory available to the
 reduce tasks (in kilo-bytes). This has been deprecated and will no
 longer have any effect.]]>
      </doc>
    </field>
    <field name="MAPRED_TASK_ENV" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="Use {@link #MAPRED_MAP_TASK_ENV} or 
                 {@link #MAPRED_REDUCE_TASK_ENV}">
      <doc>
      <![CDATA[Configuration key to set the environment of the child map/reduce tasks.
 
 The format of the value is <code>k1=v1,k2=v2</code>. Further it can 
 reference existing environment variables via <code>$key</code> on
 Linux or <code>%key%</code> on Windows.
 
 Example:
 <ul>
   <li> A=foo - This will set the env variable A to foo. </li>
 </ul>
 
 @deprecated Use {@link #MAPRED_MAP_TASK_ENV} or 
                 {@link #MAPRED_REDUCE_TASK_ENV}]]>
      </doc>
    </field>
    <field name="MAPRED_MAP_TASK_ENV" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Configuration key to set the environment of the child map tasks.
 
 The format of the value is <code>k1=v1,k2=v2</code>. Further it can
 reference existing environment variables via <code>$key</code> on
 Linux or <code>%key%</code> on Windows.
 
 Example:
 <ul>
   <li> A=foo - This will set the env variable A to foo. </li>
 </ul>

 You can also add environment variables individually by appending
 <code>.VARNAME</code> to this configuration key, where VARNAME is
 the name of the environment variable.

 Example:
 <ul>
   <li>mapreduce.map.env.VARNAME=value</li>
 </ul>]]>
      </doc>
    </field>
    <field name="MAPRED_REDUCE_TASK_ENV" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Configuration key to set the environment of the child reduce tasks.
 
 The format of the value is <code>k1=v1,k2=v2</code>. Further it can 
 reference existing environment variables via <code>$key</code> on
 Linux or <code>%key%</code> on Windows.
 
 Example:
 <ul>
   <li> A=foo - This will set the env variable A to foo. </li>
 </ul>

 You can also add environment variables individually by appending
 <code>.VARNAME</code> to this configuration key, where VARNAME is
 the name of the environment variable.

 Example:
 <ul>
   <li>mapreduce.reduce.env.VARNAME=value</li>
 </ul>]]>
      </doc>
    </field>
    <field name="MAPRED_MAP_TASK_LOG_LEVEL" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Configuration key to set the logging level for the map task.

 The allowed logging levels are:
 OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL.]]>
      </doc>
    </field>
    <field name="MAPRED_REDUCE_TASK_LOG_LEVEL" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Configuration key to set the logging level for the reduce task.

 The allowed logging levels are:
 OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL.]]>
      </doc>
    </field>
    <field name="DEFAULT_LOG_LEVEL" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Default logging level for map/reduce tasks.]]>
      </doc>
    </field>
    <field name="WORKFLOW_ID" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[The variable is kept for M/R 1.x applications, M/R 2.x applications should
 use {@link MRJobConfig#WORKFLOW_ID} instead]]>
      </doc>
    </field>
    <field name="WORKFLOW_NAME" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[The variable is kept for M/R 1.x applications, M/R 2.x applications should
 use {@link MRJobConfig#WORKFLOW_NAME} instead]]>
      </doc>
    </field>
    <field name="WORKFLOW_NODE_NAME" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[The variable is kept for M/R 1.x applications, M/R 2.x applications should
 use {@link MRJobConfig#WORKFLOW_NODE_NAME} instead]]>
      </doc>
    </field>
    <field name="WORKFLOW_ADJACENCY_PREFIX_STRING" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[The variable is kept for M/R 1.x applications, M/R 2.x applications should
 use {@link MRJobConfig#WORKFLOW_ADJACENCY_PREFIX_STRING} instead]]>
      </doc>
    </field>
    <field name="WORKFLOW_ADJACENCY_PREFIX_PATTERN" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[The variable is kept for M/R 1.x applications, M/R 2.x applications should
 use {@link MRJobConfig#WORKFLOW_ADJACENCY_PREFIX_PATTERN} instead]]>
      </doc>
    </field>
    <field name="WORKFLOW_TAGS" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[The variable is kept for M/R 1.x applications, M/R 2.x applications should
 use {@link MRJobConfig#WORKFLOW_TAGS} instead]]>
      </doc>
    </field>
    <field name="MAPREDUCE_RECOVER_JOB" type="java.lang.String"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[The variable is kept for M/R 1.x applications, M/R 2.x applications should
 not use it]]>
      </doc>
    </field>
    <field name="DEFAULT_MAPREDUCE_RECOVER_JOB" type="boolean"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[The variable is kept for M/R 1.x applications, M/R 2.x applications should
 not use it]]>
      </doc>
    </field>
    <doc>
    <![CDATA[A map/reduce job configuration.
 
 <p><code>JobConf</code> is the primary interface for a user to describe a 
 map-reduce job to the Hadoop framework for execution. The framework tries to
 faithfully execute the job as-is described by <code>JobConf</code>, however:
 <ol>
   <li>
   Some configuration parameters might have been marked as 
   <a href="{@docRoot}/org/apache/hadoop/conf/Configuration.html#FinalParams">
   final</a> by administrators and hence cannot be altered.
   </li>
   <li>
   While some job parameters are straight-forward to set 
   (e.g. {@link #setNumReduceTasks(int)}), some parameters interact subtly
   with the rest of the framework and/or job-configuration and is relatively
   more complex for the user to control finely
   (e.g. {@link #setNumMapTasks(int)}).
   </li>
 </ol>
 
 <p><code>JobConf</code> typically specifies the {@link Mapper}, combiner 
 (if any), {@link Partitioner}, {@link Reducer}, {@link InputFormat} and 
 {@link OutputFormat} implementations to be used etc.

 <p>Optionally <code>JobConf</code> is used to specify other advanced facets 
 of the job such as <code>Comparator</code>s to be used, files to be put in  
 the {@link DistributedCache}, whether or not intermediate and/or job outputs 
 are to be compressed (and how), debugability via user-provided scripts 
 ( {@link #setMapDebugScript(String)}/{@link #setReduceDebugScript(String)}),
 for doing post-processing on task logs, task's stdout, stderr, syslog. 
 and etc.</p>
 
 <p>Here is an example on how to configure a job via <code>JobConf</code>:</p>
 <p><blockquote><pre>
     // Create a new JobConf
     JobConf job = new JobConf(new Configuration(), MyJob.class);
     
     // Specify various job-specific parameters     
     job.setJobName("myjob");
     
     FileInputFormat.setInputPaths(job, new Path("in"));
     FileOutputFormat.setOutputPath(job, new Path("out"));
     
     job.setMapperClass(MyJob.MyMapper.class);
     job.setCombinerClass(MyJob.MyReducer.class);
     job.setReducerClass(MyJob.MyReducer.class);
     
     job.setInputFormat(SequenceFileInputFormat.class);
     job.setOutputFormat(SequenceFileOutputFormat.class);
 </pre></blockquote>
 
 @see JobClient
 @see ClusterStatus
 @see Tool
 @see DistributedCache]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.JobConf -->
  <!-- start interface org.apache.hadoop.mapred.JobConfigurable -->
  <interface name="JobConfigurable"    abstract="true"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <method name="configure"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <doc>
      <![CDATA[Initializes a new instance from a {@link JobConf}.

 @param job the configuration]]>
      </doc>
    </method>
    <doc>
    <![CDATA[That what may be configured.]]>
    </doc>
  </interface>
  <!-- end interface org.apache.hadoop.mapred.JobConfigurable -->
  <!-- start interface org.apache.hadoop.mapred.JobContext -->
  <interface name="JobContext"    abstract="true"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.mapreduce.JobContext"/>
    <method name="getJobConf" return="org.apache.hadoop.mapred.JobConf"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the job Configuration
 
 @return JobConf]]>
      </doc>
    </method>
    <method name="getProgressible" return="org.apache.hadoop.util.Progressable"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get the progress mechanism for reporting progress.
 
 @return progress mechanism]]>
      </doc>
    </method>
  </interface>
  <!-- end interface org.apache.hadoop.mapred.JobContext -->
  <!-- start class org.apache.hadoop.mapred.JobID -->
  <class name="JobID" extends="org.apache.hadoop.mapreduce.JobID"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <constructor name="JobID" type="java.lang.String, int"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Constructs a JobID object 
 @param jtIdentifier jobTracker identifier
 @param id job number]]>
      </doc>
    </constructor>
    <constructor name="JobID"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="downgrade" return="org.apache.hadoop.mapred.JobID"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="old" type="org.apache.hadoop.mapreduce.JobID"/>
      <doc>
      <![CDATA[Downgrade a new JobID to an old one
 @param old a new or old JobID
 @return either old or a new JobID build to match old]]>
      </doc>
    </method>
    <method name="read" return="org.apache.hadoop.mapred.JobID"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="in" type="java.io.DataInput"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="forName" return="org.apache.hadoop.mapred.JobID"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="str" type="java.lang.String"/>
      <exception name="IllegalArgumentException" type="java.lang.IllegalArgumentException"/>
      <doc>
      <![CDATA[Construct a JobId object from given string 
 @return constructed JobId object or null if the given String is null
 @throws IllegalArgumentException if the given string is malformed]]>
      </doc>
    </method>
    <method name="getJobIDsPattern" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="jtIdentifier" type="java.lang.String"/>
      <param name="jobId" type="java.lang.Integer"/>
      <doc>
      <![CDATA[Returns a regex pattern which matches task IDs. Arguments can 
 be given null, in which case that part of the regex will be generic.  
 For example to obtain a regex matching <i>any job</i> 
 run on the jobtracker started at <i>200707121733</i>, we would use :
 <pre> 
 JobID.getTaskIDsPattern("200707121733", null);
 </pre>
 which will return :
 <pre> "job_200707121733_[0-9]*" </pre> 
 @param jtIdentifier jobTracker identifier, or null
 @param jobId job number, or null
 @return a regex pattern matching JobIDs]]>
      </doc>
    </method>
    <doc>
    <![CDATA[JobID represents the immutable and unique identifier for 
 the job. JobID consists of two parts. First part 
 represents the jobtracker identifier, so that jobID to jobtracker map 
 is defined. For cluster setup this string is the jobtracker 
 start time, for local setting, it is "local".
 Second part of the JobID is the job number. <br> 
 An example JobID is : 
 <code>job_200707121733_0003</code> , which represents the third job 
 running at the jobtracker started at <code>200707121733</code>. 
 <p>
 Applications should never construct or parse JobID strings, but rather 
 use appropriate constructors or {@link #forName(String)} method. 
 
 @see TaskID
 @see TaskAttemptID]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.JobID -->
  <!-- start class org.apache.hadoop.mapred.JobPriority -->
  <class name="JobPriority" extends="java.lang.Enum"
    abstract="false"
    static="false" final="true" visibility="public"
    deprecated="not deprecated">
    <method name="values" return="org.apache.hadoop.mapred.JobPriority[]"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="valueOf" return="org.apache.hadoop.mapred.JobPriority"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="name" type="java.lang.String"/>
    </method>
    <doc>
    <![CDATA[Used to describe the priority of the running job. 
 DEFAULT : While submitting a job, if the user is not specifying priority,
 YARN has the capability to pick the default priority as per its config.
 Hence MapReduce can indicate such cases with this new enum.
 UNDEFINED_PRIORITY : YARN supports priority as an integer. Hence other than
 the five defined enums, YARN can consider other integers also. To generalize
 such cases, this specific enum is used.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.JobPriority -->
  <!-- start class org.apache.hadoop.mapred.JobQueueInfo -->
  <class name="JobQueueInfo" extends="org.apache.hadoop.mapreduce.QueueInfo"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <constructor name="JobQueueInfo"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Default constructor for Job Queue Info.]]>
      </doc>
    </constructor>
    <constructor name="JobQueueInfo" type="java.lang.String, java.lang.String"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Construct a new JobQueueInfo object using the queue name and the
 scheduling information passed.
 
 @param queueName Name of the job queue
 @param schedulingInfo Scheduling Information associated with the job
 queue]]>
      </doc>
    </constructor>
    <method name="getQueueState" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Use getState() instead]]>
      </doc>
    </method>
    <method name="getChildren" return="java.util.List"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <doc>
    <![CDATA[Class that contains the information regarding the Job Queues which are 
 maintained by the Hadoop Map/Reduce framework.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.JobQueueInfo -->
  <!-- start class org.apache.hadoop.mapred.JobStatus -->
  <class name="JobStatus" extends="org.apache.hadoop.mapreduce.JobStatus"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <constructor name="JobStatus"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <constructor name="JobStatus" type="org.apache.hadoop.mapred.JobID, float, float, float, int"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <constructor name="JobStatus" type="org.apache.hadoop.mapred.JobID, float, float, int"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Create a job status object for a given jobid.
 @param jobid The jobid of the job
 @param mapProgress The progress made on the maps
 @param reduceProgress The progress made on the reduces
 @param runState The current state of the job]]>
      </doc>
    </constructor>
    <constructor name="JobStatus" type="org.apache.hadoop.mapred.JobID, float, float, float, int, org.apache.hadoop.mapred.JobPriority"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Create a job status object for a given jobid.
 @param jobid The jobid of the job
 @param mapProgress The progress made on the maps
 @param reduceProgress The progress made on the reduces
 @param runState The current state of the job
 @param jp Priority of the job.]]>
      </doc>
    </constructor>
    <constructor name="JobStatus" type="org.apache.hadoop.mapred.JobID, float, float, float, float, int, org.apache.hadoop.mapred.JobPriority"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Create a job status object for a given jobid.
 @param jobid The jobid of the job
 @param setupProgress The progress made on the setup
 @param mapProgress The progress made on the maps
 @param reduceProgress The progress made on the reduces
 @param cleanupProgress The progress made on the cleanup
 @param runState The current state of the job
 @param jp Priority of the job.]]>
      </doc>
    </constructor>
    <constructor name="JobStatus" type="org.apache.hadoop.mapred.JobID, float, float, float, int, java.lang.String, java.lang.String, java.lang.String, java.lang.String"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Create a job status object for a given jobid.
 @param jobid The jobid of the job
 @param mapProgress The progress made on the maps
 @param reduceProgress The progress made on the reduces
 @param cleanupProgress The progress made on cleanup
 @param runState The current state of the job
 @param user userid of the person who submitted the job.
 @param jobName user-specified job name.
 @param jobFile job configuration file. 
 @param trackingUrl link to the web-ui for details of the job.]]>
      </doc>
    </constructor>
    <constructor name="JobStatus" type="org.apache.hadoop.mapred.JobID, float, float, int, java.lang.String, java.lang.String, java.lang.String, java.lang.String"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Create a job status object for a given jobid.
 @param jobid The jobid of the job
 @param mapProgress The progress made on the maps
 @param reduceProgress The progress made on the reduces
 @param runState The current state of the job
 @param user userid of the person who submitted the job.
 @param jobName user-specified job name.
 @param jobFile job configuration file. 
 @param trackingUrl link to the web-ui for details of the job.]]>
      </doc>
    </constructor>
    <constructor name="JobStatus" type="org.apache.hadoop.mapred.JobID, float, float, float, int, org.apache.hadoop.mapred.JobPriority, java.lang.String, java.lang.String, java.lang.String, java.lang.String"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Create a job status object for a given jobid.
 @param jobid The jobid of the job
 @param mapProgress The progress made on the maps
 @param reduceProgress The progress made on the reduces
 @param runState The current state of the job
 @param jp Priority of the job.
 @param user userid of the person who submitted the job.
 @param jobName user-specified job name.
 @param jobFile job configuration file. 
 @param trackingUrl link to the web-ui for details of the job.]]>
      </doc>
    </constructor>
    <constructor name="JobStatus" type="org.apache.hadoop.mapred.JobID, float, float, float, float, int, org.apache.hadoop.mapred.JobPriority, java.lang.String, java.lang.String, java.lang.String, java.lang.String"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Create a job status object for a given jobid.
 @param jobid The jobid of the job
 @param setupProgress The progress made on the setup
 @param mapProgress The progress made on the maps
 @param reduceProgress The progress made on the reduces
 @param cleanupProgress The progress made on the cleanup
 @param runState The current state of the job
 @param jp Priority of the job.
 @param user userid of the person who submitted the job.
 @param jobName user-specified job name.
 @param jobFile job configuration file.
 @param trackingUrl link to the web-ui for details of the job.]]>
      </doc>
    </constructor>
    <constructor name="JobStatus" type="org.apache.hadoop.mapred.JobID, float, float, float, float, int, org.apache.hadoop.mapred.JobPriority, java.lang.String, java.lang.String, java.lang.String, java.lang.String, boolean"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Create a job status object for a given jobid.
 @param jobid The jobid of the job
 @param setupProgress The progress made on the setup
 @param mapProgress The progress made on the maps
 @param reduceProgress The progress made on the reduces
 @param cleanupProgress The progress made on the cleanup
 @param runState The current state of the job
 @param jp Priority of the job.
 @param user userid of the person who submitted the job.
 @param jobName user-specified job name.
 @param jobFile job configuration file.
 @param trackingUrl link to the web-ui for details of the job.
 @param isUber Whether job running in uber mode]]>
      </doc>
    </constructor>
    <constructor name="JobStatus" type="org.apache.hadoop.mapred.JobID, float, float, float, float, int, org.apache.hadoop.mapred.JobPriority, java.lang.String, java.lang.String, java.lang.String, java.lang.String, boolean, java.lang.String"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Create a job status object for a given jobid.
 @param jobid The jobid of the job
 @param setupProgress The progress made on the setup
 @param mapProgress The progress made on the maps
 @param reduceProgress The progress made on the reduces
 @param cleanupProgress The progress made on the cleanup
 @param runState The current state of the job
 @param jp Priority of the job.
 @param user userid of the person who submitted the job.
 @param jobName user-specified job name.
 @param jobFile job configuration file.
 @param trackingUrl link to the web-ui for details of the job.
 @param isUber Whether job running in uber mode
 @param historyFile history file]]>
      </doc>
    </constructor>
    <constructor name="JobStatus" type="org.apache.hadoop.mapred.JobID, float, float, float, float, int, org.apache.hadoop.mapred.JobPriority, java.lang.String, java.lang.String, java.lang.String, java.lang.String, java.lang.String"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Create a job status object for a given jobid.
 @param jobid The jobid of the job
 @param setupProgress The progress made on the setup
 @param mapProgress The progress made on the maps
 @param reduceProgress The progress made on the reduces
 @param cleanupProgress The progress made on the cleanup
 @param runState The current state of the job
 @param jp Priority of the job.
 @param user userid of the person who submitted the job.
 @param jobName user-specified job name.
 @param queue job queue name.
 @param jobFile job configuration file.
 @param trackingUrl link to the web-ui for details of the job.]]>
      </doc>
    </constructor>
    <constructor name="JobStatus" type="org.apache.hadoop.mapred.JobID, float, float, float, float, int, org.apache.hadoop.mapred.JobPriority, java.lang.String, java.lang.String, java.lang.String, java.lang.String, java.lang.String, boolean"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Create a job status object for a given jobid.
 @param jobid The jobid of the job
 @param setupProgress The progress made on the setup
 @param mapProgress The progress made on the maps
 @param reduceProgress The progress made on the reduces
 @param cleanupProgress The progress made on the cleanup
 @param runState The current state of the job
 @param jp Priority of the job.
 @param user userid of the person who submitted the job.
 @param jobName user-specified job name.
 @param queue job queue name.
 @param jobFile job configuration file. 
 @param trackingUrl link to the web-ui for details of the job.
 @param isUber Whether job running in uber mode]]>
      </doc>
    </constructor>
    <constructor name="JobStatus" type="org.apache.hadoop.mapred.JobID, float, float, float, float, int, org.apache.hadoop.mapred.JobPriority, java.lang.String, java.lang.String, java.lang.String, java.lang.String, java.lang.String, boolean, java.lang.String"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Create a job status object for a given jobid.
 @param jobid The jobid of the job
 @param setupProgress The progress made on the setup
 @param mapProgress The progress made on the maps
 @param reduceProgress The progress made on the reduces
 @param cleanupProgress The progress made on the cleanup
 @param runState The current state of the job
 @param jp Priority of the job.
 @param user userid of the person who submitted the job.
 @param jobName user-specified job name.
 @param queue job queue name.
 @param jobFile job configuration file.
 @param trackingUrl link to the web-ui for details of the job.
 @param isUber Whether job running in uber mode
 @param historyFile history file]]>
      </doc>
    </constructor>
    <method name="getJobRunState" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="state" type="int"/>
      <doc>
      <![CDATA[Helper method to get human-readable state of the job.
 @param state job state
 @return human-readable state of the job]]>
      </doc>
    </method>
    <method name="downgrade" return="org.apache.hadoop.mapred.JobStatus"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="stat" type="org.apache.hadoop.mapreduce.JobStatus"/>
    </method>
    <method name="getJobId" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="use getJobID instead">
      <doc>
      <![CDATA[@deprecated use getJobID instead]]>
      </doc>
    </method>
    <method name="getJobID" return="org.apache.hadoop.mapred.JobID"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[@return The jobid of the Job]]>
      </doc>
    </method>
    <method name="getJobPriority" return="org.apache.hadoop.mapred.JobPriority"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Return the priority of the job
 @return job priority]]>
      </doc>
    </method>
    <method name="setMapProgress"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="p" type="float"/>
      <doc>
      <![CDATA[Sets the map progress of this job
 @param p The value of map progress to set to]]>
      </doc>
    </method>
    <method name="setCleanupProgress"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="p" type="float"/>
      <doc>
      <![CDATA[Sets the cleanup progress of this job
 @param p The value of cleanup progress to set to]]>
      </doc>
    </method>
    <method name="setSetupProgress"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="p" type="float"/>
      <doc>
      <![CDATA[Sets the setup progress of this job
 @param p The value of setup progress to set to]]>
      </doc>
    </method>
    <method name="setReduceProgress"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="p" type="float"/>
      <doc>
      <![CDATA[Sets the reduce progress of this Job
 @param p The value of reduce progress to set to]]>
      </doc>
    </method>
    <method name="setFinishTime"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="finishTime" type="long"/>
      <doc>
      <![CDATA[Set the finish time of the job
 @param finishTime The finishTime of the job]]>
      </doc>
    </method>
    <method name="setHistoryFile"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="historyFile" type="java.lang.String"/>
      <doc>
      <![CDATA[Set the job history file url for a completed job]]>
      </doc>
    </method>
    <method name="setTrackingUrl"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="trackingUrl" type="java.lang.String"/>
      <doc>
      <![CDATA[Set the link to the web-ui for details of the job.]]>
      </doc>
    </method>
    <method name="setRetired"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Set the job retire flag to true.]]>
      </doc>
    </method>
    <method name="getRunState" return="int"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[@return running state of the job]]>
      </doc>
    </method>
    <method name="setStartTime"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="startTime" type="long"/>
      <doc>
      <![CDATA[Set the start time of the job
 @param startTime The startTime of the job]]>
      </doc>
    </method>
    <method name="setUsername"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="userName" type="java.lang.String"/>
      <doc>
      <![CDATA[@param userName The username of the job]]>
      </doc>
    </method>
    <method name="setJobACLs"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="acls" type="java.util.Map"/>
    </method>
    <method name="setFailureInfo"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="failureInfo" type="java.lang.String"/>
    </method>
    <method name="setJobPriority"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="jp" type="org.apache.hadoop.mapred.JobPriority"/>
      <doc>
      <![CDATA[Set the priority of the job, defaulting to NORMAL.
 @param jp new job priority]]>
      </doc>
    </method>
    <method name="mapProgress" return="float"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[@return Percentage of progress in maps]]>
      </doc>
    </method>
    <method name="cleanupProgress" return="float"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[@return Percentage of progress in cleanup]]>
      </doc>
    </method>
    <method name="setupProgress" return="float"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[@return Percentage of progress in setup]]>
      </doc>
    </method>
    <method name="reduceProgress" return="float"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[@return Percentage of progress in reduce]]>
      </doc>
    </method>
    <field name="RUNNING" type="int"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <field name="SUCCEEDED" type="int"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <field name="FAILED" type="int"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <field name="PREP" type="int"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <field name="KILLED" type="int"
      transient="false" volatile="false"
      static="true" final="true" visibility="public"
      deprecated="not deprecated">
    </field>
    <doc>
    <![CDATA[Describes the current status of a job.  This is
 not intended to be a comprehensive piece of data.
 For that, look at JobProfile.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.JobStatus -->
  <!-- start class org.apache.hadoop.mapred.KeyValueLineRecordReader -->
  <class name="KeyValueLineRecordReader" extends="java.lang.Object"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.mapred.RecordReader"/>
    <constructor name="KeyValueLineRecordReader" type="org.apache.hadoop.conf.Configuration, org.apache.hadoop.mapred.FileSplit"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
    </constructor>
    <method name="getKeyClass" return="java.lang.Class"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="createKey" return="org.apache.hadoop.io.Text"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="createValue" return="org.apache.hadoop.io.Text"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </method>
    <method name="findSeparator" return="int"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="utf" type="byte[]"/>
      <param name="start" type="int"/>
      <param name="length" type="int"/>
      <param name="sep" type="byte"/>
    </method>
    <method name="next" return="boolean"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="key" type="org.apache.hadoop.io.Text"/>
      <param name="value" type="org.apache.hadoop.io.Text"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Read key/value pair in a line.]]>
      </doc>
    </method>
    <method name="getProgress" return="float"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="getPos" return="long"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="close"
      abstract="false" native="false" synchronized="true"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <doc>
    <![CDATA[This class treats a line in the input as a key/value pair separated by a 
 separator character. The separator can be specified in config file 
 under the attribute name mapreduce.input.keyvaluelinerecordreader.key.value.separator. The default
 separator is the tab character ('\t').]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.KeyValueLineRecordReader -->
  <!-- start class org.apache.hadoop.mapred.KeyValueTextInputFormat -->
  <class name="KeyValueTextInputFormat" extends="org.apache.hadoop.mapred.FileInputFormat"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.mapred.JobConfigurable"/>
    <constructor name="KeyValueTextInputFormat"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="configure"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.mapred.JobConf"/>
    </method>
    <method name="isSplitable" return="boolean"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
      <param name="fs" type="org.apache.hadoop.fs.FileSystem"/>
      <param name="file" type="org.apache.hadoop.fs.Path"/>
    </method>
    <method name="getRecordReader" return="org.apache.hadoop.mapred.RecordReader"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="genericSplit" type="org.apache.hadoop.mapred.InputSplit"/>
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="reporter" type="org.apache.hadoop.mapred.Reporter"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <doc>
    <![CDATA[An {@link InputFormat} for plain text files. Files are broken into lines.
 Either linefeed or carriage-return are used to signal end of line. Each line
 is divided into key and value parts by a separator byte. If no such a byte
 exists, the key will be the entire line and value will be empty.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.KeyValueTextInputFormat -->
  <!-- start class org.apache.hadoop.mapred.MapFileOutputFormat -->
  <class name="MapFileOutputFormat" extends="org.apache.hadoop.mapred.FileOutputFormat"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <constructor name="MapFileOutputFormat"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="getRecordWriter" return="org.apache.hadoop.mapred.RecordWriter"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="ignored" type="org.apache.hadoop.fs.FileSystem"/>
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="name" type="java.lang.String"/>
      <param name="progress" type="org.apache.hadoop.util.Progressable"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="getReaders" return="org.apache.hadoop.io.MapFile.Reader[]"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="ignored" type="org.apache.hadoop.fs.FileSystem"/>
      <param name="dir" type="org.apache.hadoop.fs.Path"/>
      <param name="conf" type="org.apache.hadoop.conf.Configuration"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Open the output generated by this format.]]>
      </doc>
    </method>
    <method name="getEntry" return="org.apache.hadoop.io.Writable"
      abstract="false" native="false" synchronized="false"
      static="true" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="readers" type="org.apache.hadoop.io.MapFile.Reader[]"/>
      <param name="partitioner" type="org.apache.hadoop.mapred.Partitioner"/>
      <param name="key" type="K"/>
      <param name="value" type="V"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Get an entry from output generated by this class.]]>
      </doc>
    </method>
    <doc>
    <![CDATA[An {@link OutputFormat} that writes {@link MapFile}s.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.MapFileOutputFormat -->
  <!-- start interface org.apache.hadoop.mapred.Mapper -->
  <interface name="Mapper"    abstract="true"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.mapred.JobConfigurable"/>
    <implements name="org.apache.hadoop.io.Closeable"/>
    <method name="map"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="key" type="K1"/>
      <param name="value" type="V1"/>
      <param name="output" type="org.apache.hadoop.mapred.OutputCollector"/>
      <param name="reporter" type="org.apache.hadoop.mapred.Reporter"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Maps a single input key/value pair into an intermediate key/value pair.
 
 <p>Output pairs need not be of the same types as input pairs.  A given 
 input pair may map to zero or many output pairs.  Output pairs are 
 collected with calls to 
 {@link OutputCollector#collect(Object,Object)}.</p>

 <p>Applications can use the {@link Reporter} provided to report progress 
 or just indicate that they are alive. In scenarios where the application 
 takes significant amount of time to process individual key/value
 pairs, this is crucial since the framework might assume that the task has 
 timed-out and kill that task. The other way of avoiding this is to set 
 <a href="{@docRoot}/../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml#mapreduce.task.timeout">
 mapreduce.task.timeout</a> to a high-enough value (or even zero for no 
 time-outs).</p>
 
 @param key the input key.
 @param value the input value.
 @param output collects mapped keys and values.
 @param reporter facility to report progress.]]>
      </doc>
    </method>
    <doc>
    <![CDATA[Maps input key/value pairs to a set of intermediate key/value pairs.  
 
 <p>Maps are the individual tasks which transform input records into a 
 intermediate records. The transformed intermediate records need not be of 
 the same type as the input records. A given input pair may map to zero or 
 many output pairs.</p> 
 
 <p>The Hadoop Map-Reduce framework spawns one map task for each 
 {@link InputSplit} generated by the {@link InputFormat} for the job.
 <code>Mapper</code> implementations can access the {@link JobConf} for the 
 job via the {@link JobConfigurable#configure(JobConf)} and initialize
 themselves. Similarly they can use the {@link Closeable#close()} method for
 de-initialization.</p>
 
 <p>The framework then calls 
 {@link #map(Object, Object, OutputCollector, Reporter)} 
 for each key/value pair in the <code>InputSplit</code> for that task.</p>
 
 <p>All intermediate values associated with a given output key are 
 subsequently grouped by the framework, and passed to a {@link Reducer} to  
 determine the final output. Users can control the grouping by specifying
 a <code>Comparator</code> via 
 {@link JobConf#setOutputKeyComparatorClass(Class)}.</p>

 <p>The grouped <code>Mapper</code> outputs are partitioned per 
 <code>Reducer</code>. Users can control which keys (and hence records) go to 
 which <code>Reducer</code> by implementing a custom {@link Partitioner}.
 
 <p>Users can optionally specify a <code>combiner</code>, via 
 {@link JobConf#setCombinerClass(Class)}, to perform local aggregation of the 
 intermediate outputs, which helps to cut down the amount of data transferred 
 from the <code>Mapper</code> to the <code>Reducer</code>.
 
 <p>The intermediate, grouped outputs are always stored in 
 {@link SequenceFile}s. Applications can specify if and how the intermediate
 outputs are to be compressed and which {@link CompressionCodec}s are to be
 used via the <code>JobConf</code>.</p>
  
 <p>If the job has 
 <a href="{@docRoot}/org/apache/hadoop/mapred/JobConf.html#ReducerNone">zero
 reduces</a> then the output of the <code>Mapper</code> is directly written
 to the {@link FileSystem} without grouping by keys.</p>
 
 <p>Example:</p>
 <p><blockquote><pre>
     public class MyMapper&lt;K extends WritableComparable, V extends Writable&gt; 
     extends MapReduceBase implements Mapper&lt;K, V, K, V&gt; {
     
       static enum MyCounters { NUM_RECORDS }
       
       private String mapTaskId;
       private String inputFile;
       private int noRecords = 0;
       
       public void configure(JobConf job) {
         mapTaskId = job.get(JobContext.TASK_ATTEMPT_ID);
         inputFile = job.get(JobContext.MAP_INPUT_FILE);
       }
       
       public void map(K key, V val,
                       OutputCollector&lt;K, V&gt; output, Reporter reporter)
       throws IOException {
         // Process the &lt;key, value&gt; pair (assume this takes a while)
         // ...
         // ...
         
         // Let the framework know that we are alive, and kicking!
         // reporter.progress();
         
         // Process some more
         // ...
         // ...
         
         // Increment the no. of &lt;key, value&gt; pairs processed
         ++noRecords;

         // Increment counters
         reporter.incrCounter(NUM_RECORDS, 1);
        
         // Every 100 records update application-level status
         if ((noRecords%100) == 0) {
           reporter.setStatus(mapTaskId + " processed " + noRecords + 
                              " from input-file: " + inputFile); 
         }
         
         // Output the result
         output.collect(key, val);
       }
     }
 </pre></blockquote>

 <p>Applications may write a custom {@link MapRunnable} to exert greater
 control on map processing e.g. multi-threaded <code>Mapper</code>s etc.</p>
 
 @see JobConf
 @see InputFormat
 @see Partitioner  
 @see Reducer
 @see MapReduceBase
 @see MapRunnable
 @see SequenceFile]]>
    </doc>
  </interface>
  <!-- end interface org.apache.hadoop.mapred.Mapper -->
  <!-- start class org.apache.hadoop.mapred.MapReduceBase -->
  <class name="MapReduceBase" extends="java.lang.Object"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.io.Closeable"/>
    <implements name="org.apache.hadoop.mapred.JobConfigurable"/>
    <constructor name="MapReduceBase"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="close"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Default implementation that does nothing.]]>
      </doc>
    </method>
    <method name="configure"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <doc>
      <![CDATA[Default implementation that does nothing.]]>
      </doc>
    </method>
    <doc>
    <![CDATA[Base class for {@link Mapper} and {@link Reducer} implementations.
 
 <p>Provides default no-op implementations for a few methods, most non-trivial
 applications need to override some of them.</p>]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.MapReduceBase -->
  <!-- start interface org.apache.hadoop.mapred.MapRunnable -->
  <interface name="MapRunnable"    abstract="true"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.mapred.JobConfigurable"/>
    <method name="run"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="input" type="org.apache.hadoop.mapred.RecordReader"/>
      <param name="output" type="org.apache.hadoop.mapred.OutputCollector"/>
      <param name="reporter" type="org.apache.hadoop.mapred.Reporter"/>
      <exception name="IOException" type="java.io.IOException"/>
      <doc>
      <![CDATA[Start mapping input <tt>&lt;key, value&gt;</tt> pairs.
  
 <p>Mapping of input records to output records is complete when this method 
 returns.</p>
 
 @param input the {@link RecordReader} to read the input records.
 @param output the {@link OutputCollector} to collect the outputrecords.
 @param reporter {@link Reporter} to report progress, status-updates etc.
 @throws IOException]]>
      </doc>
    </method>
    <doc>
    <![CDATA[Expert: Generic interface for {@link Mapper}s.
 
 <p>Custom implementations of <code>MapRunnable</code> can exert greater 
 control on map processing e.g. multi-threaded, asynchronous mappers etc.</p>
 
 @see Mapper]]>
    </doc>
  </interface>
  <!-- end interface org.apache.hadoop.mapred.MapRunnable -->
  <!-- start class org.apache.hadoop.mapred.MapRunner -->
  <class name="MapRunner" extends="java.lang.Object"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.mapred.MapRunnable"/>
    <constructor name="MapRunner"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="configure"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
    </method>
    <method name="run"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="input" type="org.apache.hadoop.mapred.RecordReader"/>
      <param name="output" type="org.apache.hadoop.mapred.OutputCollector"/>
      <param name="reporter" type="org.apache.hadoop.mapred.Reporter"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="getMapper" return="org.apache.hadoop.mapred.Mapper"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="protected"
      deprecated="not deprecated">
    </method>
    <doc>
    <![CDATA[Default {@link MapRunnable} implementation.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.MapRunner -->
  <!-- start class org.apache.hadoop.mapred.MultiFileInputFormat -->
  <class name="MultiFileInputFormat" extends="org.apache.hadoop.mapred.FileInputFormat"
    abstract="true"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <constructor name="MultiFileInputFormat"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="getSplits" return="org.apache.hadoop.mapred.InputSplit[]"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="numSplits" type="int"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <method name="getRecordReader" return="org.apache.hadoop.mapred.RecordReader"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="split" type="org.apache.hadoop.mapred.InputSplit"/>
      <param name="job" type="org.apache.hadoop.mapred.JobConf"/>
      <param name="reporter" type="org.apache.hadoop.mapred.Reporter"/>
      <exception name="IOException" type="java.io.IOException"/>
    </method>
    <doc>
    <![CDATA[An abstract {@link InputFormat} that returns {@link MultiFileSplit}'s
 in {@link #getSplits(JobConf, int)} method. Splits are constructed from 
 the files under the input paths. Each split returned contains <i>nearly</i>
 equal content length. <br>  
 Subclasses implement {@link #getRecordReader(InputSplit, JobConf, Reporter)}
 to construct <code>RecordReader</code>'s for <code>MultiFileSplit</code>'s.
 @see MultiFileSplit]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.mapred.MultiFileInputFormat -->
  <!-- start class org.apache.hadoop.mapred.MultiFileSplit -->
  <class name="MultiFileSplit" extends="org.apache.hadoop.mapred.lib.CombineFileSplit"
    abstract="false"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <constructor name="MultiFileSplit" type="org.apache.hadoop.mapred.JobConf, org.apache.hadoop.fs.Path[], long[]"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="getLocations" return="java.lang.String[]"
      abstract="false" native="false" synchronize